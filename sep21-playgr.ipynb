{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''Plan:\n0. Load libraries, \n1. load data.\n2. Preliminary EDA.\n3. Dealing with missing values, merging train and test.\n4. Feature engineering, ohc.\n5. Sample formation.\n6. Feature scaling.\n7. Model fitting.\n8. Performance evaluation.\n9. [opt] Feature importance, error analysis.\n10. Predictions.\n'''\n\n# aside:\n# when coding for interview ML purposes or Kaggle, never drop any obervations!\n# you will have to make predictions for all obs in test sample.\n\n# correct way to deal with missing obs and merge train and test samples:\n# 1. Load both samples.\n# 2. Impute missing values in both samples, using train sample to impute missing values.\n# 3. Concatentate them into df.\n\n\n### 1. Load libraries ###\n\nimport numpy as np\nimport pandas as pd\nimport os, warnings, random, time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\n\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")\n#os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:46:01.629139Z","iopub.execute_input":"2022-04-14T21:46:01.629945Z","iopub.status.idle":"2022-04-14T21:46:02.932772Z","shell.execute_reply.started":"2022-04-14T21:46:01.629695Z","shell.execute_reply":"2022-04-14T21:46:02.931958Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"### 1. Load data ###\n\ntrain = pd.read_csv('../input/tabular-playground-series-sep-2021/train.csv')\nprint(train.shape)\n\ntest = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv')\n\ntrain = train.sample(n=200000)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:46:08.445415Z","iopub.execute_input":"2022-04-14T21:46:08.445820Z","iopub.status.idle":"2022-04-14T21:46:52.001451Z","shell.execute_reply.started":"2022-04-14T21:46:08.445769Z","shell.execute_reply":"2022-04-14T21:46:52.000542Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(957919, 120)\n","output_type":"stream"}]},{"cell_type":"code","source":"### 2. pEDA ###\n\ntrain.shape\ntrain.describe()\n\n# are there numerical features?\n\nun_colval = pd.DataFrame([[x,len(train[x].unique())] for x in train.columns], columns = ['colname', 'n_unique'])\nun_colval.loc[un_colval.n_unique < 100]\n# all columns contain numerical features","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:46:55.715971Z","iopub.execute_input":"2022-04-14T21:46:55.716290Z","iopub.status.idle":"2022-04-14T21:46:57.503855Z","shell.execute_reply.started":"2022-04-14T21:46:55.716254Z","shell.execute_reply":"2022-04-14T21:46:57.503046Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"    colname  n_unique\n119   claim         2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>colname</th>\n      <th>n_unique</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>119</th>\n      <td>claim</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"### 3. Missing values ###\n\n# given huge difference in accuracy with and without missing values, i suggest creating dummies for them.\n# to write more general code, will have to create union of mis_cols for train and test.\n\ntrain_mis_cols = [col for col in train.columns if train[col].isnull().any()]\ntest_mis_cols = [col for col in test.columns if test[col].isnull().any()]\n\nfor col in train_mis_cols:\n    train[col + '_miss'] = (train[col].isnull()).astype(int)\n\nfor col in test_mis_cols:\n    test[col + '_miss'] = (test[col].isnull()).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:47:05.434847Z","iopub.execute_input":"2022-04-14T21:47:05.435133Z","iopub.status.idle":"2022-04-14T21:47:05.967777Z","shell.execute_reply.started":"2022-04-14T21:47:05.435103Z","shell.execute_reply":"2022-04-14T21:47:05.966927Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#train.describe()\n#test.describe()\n\n# imputing missing values #\n\ncolmnames = train.columns\ntest['claim'] = np.nan\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\nimp.fit(train)\ntrain = pd.DataFrame(imp.transform(train))\ntest = pd.DataFrame(imp.transform(test))\n#train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:47:08.930732Z","iopub.execute_input":"2022-04-14T21:47:08.931038Z","iopub.status.idle":"2022-04-14T21:47:18.717971Z","shell.execute_reply.started":"2022-04-14T21:47:08.931005Z","shell.execute_reply":"2022-04-14T21:47:18.717141Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train['sample'] = 'train'\ntest['sample'] = 'pred'\ndf = pd.concat([train, test])\n#df.colnames = list(colmnames)+'sample'\ndf.reset_index(drop=True, inplace=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:47:18.719954Z","iopub.execute_input":"2022-04-14T21:47:18.720260Z","iopub.status.idle":"2022-04-14T21:47:19.295741Z","shell.execute_reply.started":"2022-04-14T21:47:18.720220Z","shell.execute_reply":"2022-04-14T21:47:19.294893Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                0         1         2         3         4         5         6  \\\n0        117075.0  0.081362  0.473380   -291.92 -0.003355  0.356400 -0.740850   \n1        822015.0  0.060645  0.465470  22233.00  0.044649  0.373000 -1.153800   \n2        477450.0  0.100570  0.003177    343.62  0.005835 -0.001988 -0.772670   \n3        631308.0  0.082687  0.162730   1475.00  0.052222  0.389130  1.481100   \n4        907459.0  0.062238  0.240750    152.68  0.098153  0.503840  0.001133   \n...           ...       ...       ...       ...       ...       ...       ...   \n693469  1451388.0 -0.009112  0.308190    637.64  0.778200  0.414150 -1.068500   \n693470  1451389.0  0.088922  0.482650   6924.10  0.025963  0.355400 -0.870200   \n693471  1451390.0  0.140620  0.484750   1797.10  0.147020  0.288030 -1.407100   \n693472  1451391.0  0.168000  0.351760    454.79  0.164580  0.169830  0.323850   \n693473  1451392.0  0.093079  0.501860   3322.40  0.137230  0.332690  0.056827   \n\n              7          8             9  ...  229  230  231  232  233  234  \\\n0       1663.60   239770.0 -3.200400e+09  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n1       2182.20   636730.0  4.636400e+15  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n2       1016.50   437150.0 -7.275500e+12  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n3       1569.60  1200800.0  3.701800e+12  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n4       1815.20   237780.0  1.623700e+15  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...        ...           ...  ...  ...  ...  ...  ...  ...  ...   \n693469   651.22   985000.0  6.079700e+15  ...  0.0  0.0  0.0  1.0  0.0  0.0   \n693470  2514.20    18004.0  6.073500e+14  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n693471   434.03   333050.0  2.351000e+15  ...  0.0  1.0  0.0  0.0  0.0  0.0   \n693472  2331.20   223980.0 -2.795300e+12  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n693473  2568.70    39185.0  1.489300e+14  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n\n        235  236  237  sample  \n0       0.0  0.0  0.0   train  \n1       0.0  0.0  0.0   train  \n2       0.0  0.0  0.0   train  \n3       0.0  0.0  0.0   train  \n4       0.0  0.0  0.0   train  \n...     ...  ...  ...     ...  \n693469  0.0  0.0  0.0    pred  \n693470  0.0  0.0  0.0    pred  \n693471  0.0  0.0  0.0    pred  \n693472  0.0  0.0  0.0    pred  \n693473  0.0  0.0  0.0    pred  \n\n[693474 rows x 239 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>229</th>\n      <th>230</th>\n      <th>231</th>\n      <th>232</th>\n      <th>233</th>\n      <th>234</th>\n      <th>235</th>\n      <th>236</th>\n      <th>237</th>\n      <th>sample</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>117075.0</td>\n      <td>0.081362</td>\n      <td>0.473380</td>\n      <td>-291.92</td>\n      <td>-0.003355</td>\n      <td>0.356400</td>\n      <td>-0.740850</td>\n      <td>1663.60</td>\n      <td>239770.0</td>\n      <td>-3.200400e+09</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>822015.0</td>\n      <td>0.060645</td>\n      <td>0.465470</td>\n      <td>22233.00</td>\n      <td>0.044649</td>\n      <td>0.373000</td>\n      <td>-1.153800</td>\n      <td>2182.20</td>\n      <td>636730.0</td>\n      <td>4.636400e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>477450.0</td>\n      <td>0.100570</td>\n      <td>0.003177</td>\n      <td>343.62</td>\n      <td>0.005835</td>\n      <td>-0.001988</td>\n      <td>-0.772670</td>\n      <td>1016.50</td>\n      <td>437150.0</td>\n      <td>-7.275500e+12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>631308.0</td>\n      <td>0.082687</td>\n      <td>0.162730</td>\n      <td>1475.00</td>\n      <td>0.052222</td>\n      <td>0.389130</td>\n      <td>1.481100</td>\n      <td>1569.60</td>\n      <td>1200800.0</td>\n      <td>3.701800e+12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>907459.0</td>\n      <td>0.062238</td>\n      <td>0.240750</td>\n      <td>152.68</td>\n      <td>0.098153</td>\n      <td>0.503840</td>\n      <td>0.001133</td>\n      <td>1815.20</td>\n      <td>237780.0</td>\n      <td>1.623700e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>693469</th>\n      <td>1451388.0</td>\n      <td>-0.009112</td>\n      <td>0.308190</td>\n      <td>637.64</td>\n      <td>0.778200</td>\n      <td>0.414150</td>\n      <td>-1.068500</td>\n      <td>651.22</td>\n      <td>985000.0</td>\n      <td>6.079700e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>693470</th>\n      <td>1451389.0</td>\n      <td>0.088922</td>\n      <td>0.482650</td>\n      <td>6924.10</td>\n      <td>0.025963</td>\n      <td>0.355400</td>\n      <td>-0.870200</td>\n      <td>2514.20</td>\n      <td>18004.0</td>\n      <td>6.073500e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>693471</th>\n      <td>1451390.0</td>\n      <td>0.140620</td>\n      <td>0.484750</td>\n      <td>1797.10</td>\n      <td>0.147020</td>\n      <td>0.288030</td>\n      <td>-1.407100</td>\n      <td>434.03</td>\n      <td>333050.0</td>\n      <td>2.351000e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>693472</th>\n      <td>1451391.0</td>\n      <td>0.168000</td>\n      <td>0.351760</td>\n      <td>454.79</td>\n      <td>0.164580</td>\n      <td>0.169830</td>\n      <td>0.323850</td>\n      <td>2331.20</td>\n      <td>223980.0</td>\n      <td>-2.795300e+12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>693473</th>\n      <td>1451392.0</td>\n      <td>0.093079</td>\n      <td>0.501860</td>\n      <td>3322.40</td>\n      <td>0.137230</td>\n      <td>0.332690</td>\n      <td>0.056827</td>\n      <td>2568.70</td>\n      <td>39185.0</td>\n      <td>1.489300e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n  </tbody>\n</table>\n<p>693474 rows × 239 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"newcolnames = list(colmnames) + ['sample']\ndf.columns = newcolnames\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:47:24.833699Z","iopub.execute_input":"2022-04-14T21:47:24.834622Z","iopub.status.idle":"2022-04-14T21:47:24.976360Z","shell.execute_reply.started":"2022-04-14T21:47:24.834566Z","shell.execute_reply":"2022-04-14T21:47:24.975552Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"               id        f1        f2        f3        f4        f5        f6  \\\n0        117075.0  0.081362  0.473380   -291.92 -0.003355  0.356400 -0.740850   \n1        822015.0  0.060645  0.465470  22233.00  0.044649  0.373000 -1.153800   \n2        477450.0  0.100570  0.003177    343.62  0.005835 -0.001988 -0.772670   \n3        631308.0  0.082687  0.162730   1475.00  0.052222  0.389130  1.481100   \n4        907459.0  0.062238  0.240750    152.68  0.098153  0.503840  0.001133   \n...           ...       ...       ...       ...       ...       ...       ...   \n693469  1451388.0 -0.009112  0.308190    637.64  0.778200  0.414150 -1.068500   \n693470  1451389.0  0.088922  0.482650   6924.10  0.025963  0.355400 -0.870200   \n693471  1451390.0  0.140620  0.484750   1797.10  0.147020  0.288030 -1.407100   \n693472  1451391.0  0.168000  0.351760    454.79  0.164580  0.169830  0.323850   \n693473  1451392.0  0.093079  0.501860   3322.40  0.137230  0.332690  0.056827   \n\n             f7         f8            f9  ...  f110_miss  f111_miss  \\\n0       1663.60   239770.0 -3.200400e+09  ...        0.0        0.0   \n1       2182.20   636730.0  4.636400e+15  ...        0.0        0.0   \n2       1016.50   437150.0 -7.275500e+12  ...        0.0        0.0   \n3       1569.60  1200800.0  3.701800e+12  ...        0.0        0.0   \n4       1815.20   237780.0  1.623700e+15  ...        0.0        0.0   \n...         ...        ...           ...  ...        ...        ...   \n693469   651.22   985000.0  6.079700e+15  ...        0.0        0.0   \n693470  2514.20    18004.0  6.073500e+14  ...        0.0        0.0   \n693471   434.03   333050.0  2.351000e+15  ...        0.0        1.0   \n693472  2331.20   223980.0 -2.795300e+12  ...        0.0        0.0   \n693473  2568.70    39185.0  1.489300e+14  ...        0.0        0.0   \n\n        f112_miss  f113_miss  f114_miss  f115_miss  f116_miss  f117_miss  \\\n0             0.0        0.0        0.0        0.0        0.0        0.0   \n1             0.0        0.0        0.0        0.0        0.0        0.0   \n2             0.0        0.0        0.0        0.0        0.0        0.0   \n3             0.0        0.0        0.0        0.0        0.0        0.0   \n4             0.0        0.0        0.0        0.0        0.0        0.0   \n...           ...        ...        ...        ...        ...        ...   \n693469        0.0        1.0        0.0        0.0        0.0        0.0   \n693470        0.0        0.0        0.0        0.0        0.0        0.0   \n693471        0.0        0.0        0.0        0.0        0.0        0.0   \n693472        0.0        0.0        0.0        0.0        0.0        0.0   \n693473        0.0        0.0        0.0        0.0        0.0        0.0   \n\n        f118_miss  sample  \n0             0.0   train  \n1             0.0   train  \n2             0.0   train  \n3             0.0   train  \n4             0.0   train  \n...           ...     ...  \n693469        0.0    pred  \n693470        0.0    pred  \n693471        0.0    pred  \n693472        0.0    pred  \n693473        0.0    pred  \n\n[693474 rows x 239 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>...</th>\n      <th>f110_miss</th>\n      <th>f111_miss</th>\n      <th>f112_miss</th>\n      <th>f113_miss</th>\n      <th>f114_miss</th>\n      <th>f115_miss</th>\n      <th>f116_miss</th>\n      <th>f117_miss</th>\n      <th>f118_miss</th>\n      <th>sample</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>117075.0</td>\n      <td>0.081362</td>\n      <td>0.473380</td>\n      <td>-291.92</td>\n      <td>-0.003355</td>\n      <td>0.356400</td>\n      <td>-0.740850</td>\n      <td>1663.60</td>\n      <td>239770.0</td>\n      <td>-3.200400e+09</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>822015.0</td>\n      <td>0.060645</td>\n      <td>0.465470</td>\n      <td>22233.00</td>\n      <td>0.044649</td>\n      <td>0.373000</td>\n      <td>-1.153800</td>\n      <td>2182.20</td>\n      <td>636730.0</td>\n      <td>4.636400e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>477450.0</td>\n      <td>0.100570</td>\n      <td>0.003177</td>\n      <td>343.62</td>\n      <td>0.005835</td>\n      <td>-0.001988</td>\n      <td>-0.772670</td>\n      <td>1016.50</td>\n      <td>437150.0</td>\n      <td>-7.275500e+12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>631308.0</td>\n      <td>0.082687</td>\n      <td>0.162730</td>\n      <td>1475.00</td>\n      <td>0.052222</td>\n      <td>0.389130</td>\n      <td>1.481100</td>\n      <td>1569.60</td>\n      <td>1200800.0</td>\n      <td>3.701800e+12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>907459.0</td>\n      <td>0.062238</td>\n      <td>0.240750</td>\n      <td>152.68</td>\n      <td>0.098153</td>\n      <td>0.503840</td>\n      <td>0.001133</td>\n      <td>1815.20</td>\n      <td>237780.0</td>\n      <td>1.623700e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>693469</th>\n      <td>1451388.0</td>\n      <td>-0.009112</td>\n      <td>0.308190</td>\n      <td>637.64</td>\n      <td>0.778200</td>\n      <td>0.414150</td>\n      <td>-1.068500</td>\n      <td>651.22</td>\n      <td>985000.0</td>\n      <td>6.079700e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>693470</th>\n      <td>1451389.0</td>\n      <td>0.088922</td>\n      <td>0.482650</td>\n      <td>6924.10</td>\n      <td>0.025963</td>\n      <td>0.355400</td>\n      <td>-0.870200</td>\n      <td>2514.20</td>\n      <td>18004.0</td>\n      <td>6.073500e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>693471</th>\n      <td>1451390.0</td>\n      <td>0.140620</td>\n      <td>0.484750</td>\n      <td>1797.10</td>\n      <td>0.147020</td>\n      <td>0.288030</td>\n      <td>-1.407100</td>\n      <td>434.03</td>\n      <td>333050.0</td>\n      <td>2.351000e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>693472</th>\n      <td>1451391.0</td>\n      <td>0.168000</td>\n      <td>0.351760</td>\n      <td>454.79</td>\n      <td>0.164580</td>\n      <td>0.169830</td>\n      <td>0.323850</td>\n      <td>2331.20</td>\n      <td>223980.0</td>\n      <td>-2.795300e+12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>693473</th>\n      <td>1451392.0</td>\n      <td>0.093079</td>\n      <td>0.501860</td>\n      <td>3322.40</td>\n      <td>0.137230</td>\n      <td>0.332690</td>\n      <td>0.056827</td>\n      <td>2568.70</td>\n      <td>39185.0</td>\n      <td>1.489300e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n  </tbody>\n</table>\n<p>693474 rows × 239 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# we have many variables, are all of them useful? lets see correlation\n\n#cors = df.corr()\n#cors.loc[((cors > 0.8)&(cors<1)).any(1)]\n# there are no pairwise correlations above 80%","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:47:28.856413Z","iopub.execute_input":"2022-04-14T21:47:28.856688Z","iopub.status.idle":"2022-04-14T21:47:28.861258Z","shell.execute_reply.started":"2022-04-14T21:47:28.856656Z","shell.execute_reply":"2022-04-14T21:47:28.860107Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"### 4. Feature engineering ###\n\n# check skew and possibly transform some variables #\n\ntemp = (df.dtypes == np.float64)\nnum_cols = df.columns[temp]\nskew_vals = df[num_cols].skew() \nskew_limit = 1\n    \nskew_cols = (skew_vals\n             .sort_values(ascending=False)\n             .to_frame()\n             .rename(columns={0:'Skew'})\n             .query('abs(Skew) > {}'.format(skew_limit)))\n\nprint(skew_cols)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:47:31.783303Z","iopub.execute_input":"2022-04-14T21:47:31.784130Z","iopub.status.idle":"2022-04-14T21:47:34.755410Z","shell.execute_reply.started":"2022-04-14T21:47:31.784084Z","shell.execute_reply":"2022-04-14T21:47:34.754584Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"                Skew\nf118_miss  14.568339\nf2_miss     7.797908\nf13_miss    7.773938\nf80_miss    7.771329\nf112_miss   7.766118\n...              ...\nf13        -1.295357\nf110       -1.330498\nf58        -1.357247\nf46        -1.542950\nf91        -1.557878\n\n[187 rows x 1 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"### 5. ###\n\ny_train = df.loc[df['sample']=='train',['claim']]\nX_train_ = df.drop(columns=['claim'])\nX_train = X_train_.loc[X_train_['sample']=='train']\nX_pred = X_train_.loc[X_train_['sample']=='pred']\n\nprint(y_train.shape, X_train.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=2)\n\nprint(y_train.shape, X_train.shape, X_pred.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:48:55.277880Z","iopub.execute_input":"2022-04-14T21:48:55.278173Z","iopub.status.idle":"2022-04-14T21:48:56.905043Z","shell.execute_reply.started":"2022-04-14T21:48:55.278140Z","shell.execute_reply":"2022-04-14T21:48:56.904212Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"(200000, 1) (200000, 238)\n(180000, 1) (180000, 238) (493474, 238)\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train.drop(columns = ['sample'], inplace=True)\nX_test.drop(columns = ['sample'], inplace=True)\nX_pred.drop(columns = ['sample'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:49:09.889710Z","iopub.execute_input":"2022-04-14T21:49:09.890001Z","iopub.status.idle":"2022-04-14T21:49:10.280268Z","shell.execute_reply.started":"2022-04-14T21:49:09.889967Z","shell.execute_reply":"2022-04-14T21:49:10.279402Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_pred","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:49:12.507178Z","iopub.execute_input":"2022-04-14T21:49:12.508104Z","iopub.status.idle":"2022-04-14T21:49:12.583349Z","shell.execute_reply.started":"2022-04-14T21:49:12.508062Z","shell.execute_reply":"2022-04-14T21:49:12.582448Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"               id        f1        f2        f3        f4       f5        f6  \\\n200000   957919.0  0.165850  0.487050   1295.00  0.023100  0.31900  0.901880   \n200001   957920.0  0.129650  0.373480   1763.00  0.728840  0.33247 -1.263100   \n200002   957921.0  0.120190  0.445210    736.26  0.046150  0.29605  0.316650   \n200003   957922.0  0.054008  0.395960    996.14  0.859340  0.36678 -0.170600   \n200004   957923.0  0.079947 -0.006919  10574.00  0.348450  0.45008 -1.842000   \n...           ...       ...       ...       ...       ...      ...       ...   \n693469  1451388.0 -0.009112  0.308190    637.64  0.778200  0.41415 -1.068500   \n693470  1451389.0  0.088922  0.482650   6924.10  0.025963  0.35540 -0.870200   \n693471  1451390.0  0.140620  0.484750   1797.10  0.147020  0.28803 -1.407100   \n693472  1451391.0  0.168000  0.351760    454.79  0.164580  0.16983  0.323850   \n693473  1451392.0  0.093079  0.501860   3322.40  0.137230  0.33269  0.056827   \n\n             f7        f8            f9  ...  f109_miss  f110_miss  f111_miss  \\\n200000   573.29    3743.7  2.705700e+12  ...        0.0        0.0        0.0   \n200001   875.55  554370.0  5.955700e+14  ...        0.0        0.0        0.0   \n200002  2659.50  317140.0  3.977800e+14  ...        0.0        0.0        0.0   \n200003   386.56  325680.0 -3.432200e+13  ...        0.0        0.0        0.0   \n200004  3027.00  428150.0  9.291500e+11  ...        0.0        0.0        0.0   \n...         ...       ...           ...  ...        ...        ...        ...   \n693469   651.22  985000.0  6.079700e+15  ...        0.0        0.0        0.0   \n693470  2514.20   18004.0  6.073500e+14  ...        0.0        0.0        0.0   \n693471   434.03  333050.0  2.351000e+15  ...        0.0        0.0        1.0   \n693472  2331.20  223980.0 -2.795300e+12  ...        0.0        0.0        0.0   \n693473  2568.70   39185.0  1.489300e+14  ...        0.0        0.0        0.0   \n\n        f112_miss  f113_miss  f114_miss  f115_miss  f116_miss  f117_miss  \\\n200000        0.0        0.0        0.0        0.0        0.0        0.0   \n200001        0.0        0.0        0.0        0.0        0.0        0.0   \n200002        0.0        0.0        0.0        0.0        0.0        0.0   \n200003        0.0        0.0        0.0        0.0        0.0        0.0   \n200004        0.0        0.0        0.0        0.0        0.0        0.0   \n...           ...        ...        ...        ...        ...        ...   \n693469        0.0        1.0        0.0        0.0        0.0        0.0   \n693470        0.0        0.0        0.0        0.0        0.0        0.0   \n693471        0.0        0.0        0.0        0.0        0.0        0.0   \n693472        0.0        0.0        0.0        0.0        0.0        0.0   \n693473        0.0        0.0        0.0        0.0        0.0        0.0   \n\n        f118_miss  \n200000        0.0  \n200001        0.0  \n200002        0.0  \n200003        0.0  \n200004        0.0  \n...           ...  \n693469        0.0  \n693470        0.0  \n693471        0.0  \n693472        0.0  \n693473        0.0  \n\n[493474 rows x 237 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>...</th>\n      <th>f109_miss</th>\n      <th>f110_miss</th>\n      <th>f111_miss</th>\n      <th>f112_miss</th>\n      <th>f113_miss</th>\n      <th>f114_miss</th>\n      <th>f115_miss</th>\n      <th>f116_miss</th>\n      <th>f117_miss</th>\n      <th>f118_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>200000</th>\n      <td>957919.0</td>\n      <td>0.165850</td>\n      <td>0.487050</td>\n      <td>1295.00</td>\n      <td>0.023100</td>\n      <td>0.31900</td>\n      <td>0.901880</td>\n      <td>573.29</td>\n      <td>3743.7</td>\n      <td>2.705700e+12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>200001</th>\n      <td>957920.0</td>\n      <td>0.129650</td>\n      <td>0.373480</td>\n      <td>1763.00</td>\n      <td>0.728840</td>\n      <td>0.33247</td>\n      <td>-1.263100</td>\n      <td>875.55</td>\n      <td>554370.0</td>\n      <td>5.955700e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>200002</th>\n      <td>957921.0</td>\n      <td>0.120190</td>\n      <td>0.445210</td>\n      <td>736.26</td>\n      <td>0.046150</td>\n      <td>0.29605</td>\n      <td>0.316650</td>\n      <td>2659.50</td>\n      <td>317140.0</td>\n      <td>3.977800e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>200003</th>\n      <td>957922.0</td>\n      <td>0.054008</td>\n      <td>0.395960</td>\n      <td>996.14</td>\n      <td>0.859340</td>\n      <td>0.36678</td>\n      <td>-0.170600</td>\n      <td>386.56</td>\n      <td>325680.0</td>\n      <td>-3.432200e+13</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>200004</th>\n      <td>957923.0</td>\n      <td>0.079947</td>\n      <td>-0.006919</td>\n      <td>10574.00</td>\n      <td>0.348450</td>\n      <td>0.45008</td>\n      <td>-1.842000</td>\n      <td>3027.00</td>\n      <td>428150.0</td>\n      <td>9.291500e+11</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>693469</th>\n      <td>1451388.0</td>\n      <td>-0.009112</td>\n      <td>0.308190</td>\n      <td>637.64</td>\n      <td>0.778200</td>\n      <td>0.41415</td>\n      <td>-1.068500</td>\n      <td>651.22</td>\n      <td>985000.0</td>\n      <td>6.079700e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>693470</th>\n      <td>1451389.0</td>\n      <td>0.088922</td>\n      <td>0.482650</td>\n      <td>6924.10</td>\n      <td>0.025963</td>\n      <td>0.35540</td>\n      <td>-0.870200</td>\n      <td>2514.20</td>\n      <td>18004.0</td>\n      <td>6.073500e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>693471</th>\n      <td>1451390.0</td>\n      <td>0.140620</td>\n      <td>0.484750</td>\n      <td>1797.10</td>\n      <td>0.147020</td>\n      <td>0.28803</td>\n      <td>-1.407100</td>\n      <td>434.03</td>\n      <td>333050.0</td>\n      <td>2.351000e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>693472</th>\n      <td>1451391.0</td>\n      <td>0.168000</td>\n      <td>0.351760</td>\n      <td>454.79</td>\n      <td>0.164580</td>\n      <td>0.16983</td>\n      <td>0.323850</td>\n      <td>2331.20</td>\n      <td>223980.0</td>\n      <td>-2.795300e+12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>693473</th>\n      <td>1451392.0</td>\n      <td>0.093079</td>\n      <td>0.501860</td>\n      <td>3322.40</td>\n      <td>0.137230</td>\n      <td>0.33269</td>\n      <td>0.056827</td>\n      <td>2568.70</td>\n      <td>39185.0</td>\n      <td>1.489300e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>493474 rows × 237 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"### 6. Scaling ###\n\nss = StandardScaler()\n\n#for i in X_train.columns[0:119]:\nfor i in X_train.columns:\n    X_train[[i]] = ss.fit_transform(X_train[[i]])\n    X_test[[i]] = ss.transform(X_test[[i]])\n\nX_train.describe()  ","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:49:16.317246Z","iopub.execute_input":"2022-04-14T21:49:16.317696Z","iopub.status.idle":"2022-04-14T21:49:19.957745Z","shell.execute_reply.started":"2022-04-14T21:49:16.317658Z","shell.execute_reply":"2022-04-14T21:49:19.957102Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                 id            f1            f2            f3            f4  \\\ncount  1.800000e+05  1.800000e+05  1.800000e+05  1.800000e+05  1.800000e+05   \nmean   4.856177e-17  5.117993e-18 -1.955584e-16  2.022478e-16 -6.465544e-17   \nstd    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \nmin   -1.730933e+00 -5.363099e+00 -2.511483e+00 -2.071116e+00 -1.236648e+00   \n25%   -8.677114e-01 -4.570095e-01 -4.198685e-01 -5.630091e-01 -7.783965e-01   \n50%    2.135542e-03 -1.754528e-03  2.932210e-01 -4.294010e-01 -2.991274e-01   \n75%    8.640951e-01  5.842382e-01  7.617377e-01  4.715925e-02  4.455957e-01   \nmax    1.730478e+00  7.156427e+00  1.166943e+00  5.596314e+00  5.251886e+00   \n\n                 f5            f6            f7            f8            f9  \\\ncount  1.800000e+05  1.800000e+05  1.800000e+05  1.800000e+05  1.800000e+05   \nmean   2.479239e-16  4.142026e-17  3.589073e-17  2.762499e-16  1.272439e-17   \nstd    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \nmin   -2.161083e+00 -5.686915e+00 -1.438933e+00 -1.178798e+00 -1.239681e+00   \n25%   -4.351244e-01 -4.897308e-01 -8.881558e-01 -8.227977e-01 -7.612088e-01   \n50%    1.557760e-01 -1.441352e-01 -1.366565e-01 -2.509927e-01 -5.530376e-01   \n75%    7.373053e-01  4.644700e-01  6.743857e-01  5.230690e-01  5.376001e-01   \nmax    1.690007e+00  5.222199e+00  2.955129e+00  4.473430e+00  3.725684e+00   \n\n       ...     f109_miss     f110_miss     f111_miss     f112_miss  \\\ncount  ...  1.800000e+05  1.800000e+05  1.800000e+05  1.800000e+05   \nmean   ...  1.563633e-15  1.047261e-15 -2.893582e-15  2.495735e-16   \nstd    ...  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \nmin    ... -1.280766e-01 -1.279870e-01 -1.279197e-01 -1.278075e-01   \n25%    ... -1.280766e-01 -1.279870e-01 -1.279197e-01 -1.278075e-01   \n50%    ... -1.280766e-01 -1.279870e-01 -1.279197e-01 -1.278075e-01   \n75%    ... -1.280766e-01 -1.279870e-01 -1.279197e-01 -1.278075e-01   \nmax    ...  7.807825e+00  7.813294e+00  7.817404e+00  7.824267e+00   \n\n          f113_miss     f114_miss     f115_miss     f116_miss     f117_miss  \\\ncount  1.800000e+05  1.800000e+05  1.800000e+05  1.800000e+05  1.800000e+05   \nmean  -3.813125e-15 -3.509097e-16  5.098667e-15  2.325263e-16  1.170454e-14   \nstd    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \nmin   -1.260223e-01 -1.286133e-01 -1.288586e-01 -1.291479e-01 -1.297469e-01   \n25%   -1.260223e-01 -1.286133e-01 -1.288586e-01 -1.291479e-01 -1.297469e-01   \n50%   -1.260223e-01 -1.286133e-01 -1.288586e-01 -1.291479e-01 -1.297469e-01   \n75%   -1.260223e-01 -1.286133e-01 -1.288586e-01 -1.291479e-01 -1.297469e-01   \nmax    7.935105e+00  7.775244e+00  7.760444e+00  7.743059e+00  7.707314e+00   \n\n          f118_miss  \ncount  1.800000e+05  \nmean   1.187001e-16  \nstd    1.000003e+00  \nmin   -1.278524e-01  \n25%   -1.278524e-01  \n50%   -1.278524e-01  \n75%   -1.278524e-01  \nmax    7.821519e+00  \n\n[8 rows x 237 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>...</th>\n      <th>f109_miss</th>\n      <th>f110_miss</th>\n      <th>f111_miss</th>\n      <th>f112_miss</th>\n      <th>f113_miss</th>\n      <th>f114_miss</th>\n      <th>f115_miss</th>\n      <th>f116_miss</th>\n      <th>f117_miss</th>\n      <th>f118_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>...</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n      <td>1.800000e+05</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>4.856177e-17</td>\n      <td>5.117993e-18</td>\n      <td>-1.955584e-16</td>\n      <td>2.022478e-16</td>\n      <td>-6.465544e-17</td>\n      <td>2.479239e-16</td>\n      <td>4.142026e-17</td>\n      <td>3.589073e-17</td>\n      <td>2.762499e-16</td>\n      <td>1.272439e-17</td>\n      <td>...</td>\n      <td>1.563633e-15</td>\n      <td>1.047261e-15</td>\n      <td>-2.893582e-15</td>\n      <td>2.495735e-16</td>\n      <td>-3.813125e-15</td>\n      <td>-3.509097e-16</td>\n      <td>5.098667e-15</td>\n      <td>2.325263e-16</td>\n      <td>1.170454e-14</td>\n      <td>1.187001e-16</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>...</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n      <td>1.000003e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.730933e+00</td>\n      <td>-5.363099e+00</td>\n      <td>-2.511483e+00</td>\n      <td>-2.071116e+00</td>\n      <td>-1.236648e+00</td>\n      <td>-2.161083e+00</td>\n      <td>-5.686915e+00</td>\n      <td>-1.438933e+00</td>\n      <td>-1.178798e+00</td>\n      <td>-1.239681e+00</td>\n      <td>...</td>\n      <td>-1.280766e-01</td>\n      <td>-1.279870e-01</td>\n      <td>-1.279197e-01</td>\n      <td>-1.278075e-01</td>\n      <td>-1.260223e-01</td>\n      <td>-1.286133e-01</td>\n      <td>-1.288586e-01</td>\n      <td>-1.291479e-01</td>\n      <td>-1.297469e-01</td>\n      <td>-1.278524e-01</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-8.677114e-01</td>\n      <td>-4.570095e-01</td>\n      <td>-4.198685e-01</td>\n      <td>-5.630091e-01</td>\n      <td>-7.783965e-01</td>\n      <td>-4.351244e-01</td>\n      <td>-4.897308e-01</td>\n      <td>-8.881558e-01</td>\n      <td>-8.227977e-01</td>\n      <td>-7.612088e-01</td>\n      <td>...</td>\n      <td>-1.280766e-01</td>\n      <td>-1.279870e-01</td>\n      <td>-1.279197e-01</td>\n      <td>-1.278075e-01</td>\n      <td>-1.260223e-01</td>\n      <td>-1.286133e-01</td>\n      <td>-1.288586e-01</td>\n      <td>-1.291479e-01</td>\n      <td>-1.297469e-01</td>\n      <td>-1.278524e-01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.135542e-03</td>\n      <td>-1.754528e-03</td>\n      <td>2.932210e-01</td>\n      <td>-4.294010e-01</td>\n      <td>-2.991274e-01</td>\n      <td>1.557760e-01</td>\n      <td>-1.441352e-01</td>\n      <td>-1.366565e-01</td>\n      <td>-2.509927e-01</td>\n      <td>-5.530376e-01</td>\n      <td>...</td>\n      <td>-1.280766e-01</td>\n      <td>-1.279870e-01</td>\n      <td>-1.279197e-01</td>\n      <td>-1.278075e-01</td>\n      <td>-1.260223e-01</td>\n      <td>-1.286133e-01</td>\n      <td>-1.288586e-01</td>\n      <td>-1.291479e-01</td>\n      <td>-1.297469e-01</td>\n      <td>-1.278524e-01</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8.640951e-01</td>\n      <td>5.842382e-01</td>\n      <td>7.617377e-01</td>\n      <td>4.715925e-02</td>\n      <td>4.455957e-01</td>\n      <td>7.373053e-01</td>\n      <td>4.644700e-01</td>\n      <td>6.743857e-01</td>\n      <td>5.230690e-01</td>\n      <td>5.376001e-01</td>\n      <td>...</td>\n      <td>-1.280766e-01</td>\n      <td>-1.279870e-01</td>\n      <td>-1.279197e-01</td>\n      <td>-1.278075e-01</td>\n      <td>-1.260223e-01</td>\n      <td>-1.286133e-01</td>\n      <td>-1.288586e-01</td>\n      <td>-1.291479e-01</td>\n      <td>-1.297469e-01</td>\n      <td>-1.278524e-01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.730478e+00</td>\n      <td>7.156427e+00</td>\n      <td>1.166943e+00</td>\n      <td>5.596314e+00</td>\n      <td>5.251886e+00</td>\n      <td>1.690007e+00</td>\n      <td>5.222199e+00</td>\n      <td>2.955129e+00</td>\n      <td>4.473430e+00</td>\n      <td>3.725684e+00</td>\n      <td>...</td>\n      <td>7.807825e+00</td>\n      <td>7.813294e+00</td>\n      <td>7.817404e+00</td>\n      <td>7.824267e+00</td>\n      <td>7.935105e+00</td>\n      <td>7.775244e+00</td>\n      <td>7.760444e+00</td>\n      <td>7.743059e+00</td>\n      <td>7.707314e+00</td>\n      <td>7.821519e+00</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 237 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"### 7. Model fitting ###\n\ntime1 = time.time()\n\nlog_rg = LogisticRegression()\n\ngrid_values = {'penalty': ['l2'], 'C': [0.1, 0.2, 0.5, 1, 2, 3]}\n\nlr = GridSearchCV(log_rg, param_grid = grid_values, cv=4)\nlr.fit(X_train, y_train)\n\nprint('logistic', lr.best_score_, lr.best_params_, time.time()-time1)\n# after dropping missing I had 86%. while imputing them with median, i had 52%. with miss dummies, 73%.","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:50:13.332610Z","iopub.execute_input":"2022-04-14T21:50:13.332919Z","iopub.status.idle":"2022-04-14T21:50:34.357484Z","shell.execute_reply.started":"2022-04-14T21:50:13.332881Z","shell.execute_reply":"2022-04-14T21:50:34.356566Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"logistic 0.7341888888888889 {'C': 0.2, 'penalty': 'l2'} 21.016772747039795\n","output_type":"stream"}]},{"cell_type":"code","source":"time1 = time.time()\n\nsvm = svm.SVC(kernel='rbf')\n\ngrid_values = {'C':[0.01, 0.1, 1, 5, 10]}\n\nsvm = GridSearchCV(svm, param_grid = grid_values, cv=2)\nsvm.fit(X_train, y_train)\n\nprint('SVM', svm.best_score_, svm.best_params_, time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T19:36:36.268332Z","iopub.execute_input":"2022-04-14T19:36:36.268883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time1 = time.time()\n\nxgbcl = XGBClassifier(tree_method='gpu_hist', gpu_id=0)\n\ngrid_values = {'n_estimators':[100,200,300,400],'eta':[0.05, 0.8, 0.11], 'max_depth':[2,3]}\n\nxgb = GridSearchCV(xgbcl, param_grid = grid_values, cv=2)\nxgb.fit(X_train, y_train)\n\nprint('XGBoost', xgb.best_score_, xgb.best_params_, time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:50:59.627377Z","iopub.execute_input":"2022-04-14T21:50:59.628101Z","iopub.status.idle":"2022-04-14T21:52:53.317806Z","shell.execute_reply.started":"2022-04-14T21:50:59.628059Z","shell.execute_reply":"2022-04-14T21:52:53.316919Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[21:51:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:51:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[21:52:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nXGBoost 0.7315333333333334 {'eta': 0.11, 'max_depth': 3, 'n_estimators': 300} 113.68186163902283\n","output_type":"stream"}]},{"cell_type":"code","source":"### 8. performance evaluation ###\n\nyhat_lr = lr.predict(X_test)\nyhat_bt = xgb.predict(X_test)\n\nprint('Accuracy of logistic regression is ', 1-(np.abs(yhat_lr-np.array(y_test.claim))).mean())\nprint('Accuracy of XGBoost is ', 1-(np.abs(yhat_bt-np.array(y_test.claim))).mean())\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:53:04.147098Z","iopub.execute_input":"2022-04-14T21:53:04.147667Z","iopub.status.idle":"2022-04-14T21:53:04.380154Z","shell.execute_reply.started":"2022-04-14T21:53:04.147623Z","shell.execute_reply":"2022-04-14T21:53:04.379569Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Accuracy of logistic regression is  0.7323999999999999\nAccuracy of XGBoost is  0.73735\n","output_type":"stream"}]},{"cell_type":"code","source":"### Export results ###\nyhat_lr = lr.predict(X_pred).astype(int)\nyhat_bt = xgb.predict(X_pred).astype(int)\n\nsubmission_df_lr = pd.DataFrame({'id': (X_pred.id).astype(int), 'claim': yhat_lr}, columns=['id', 'claim'])\nsubmission_df_bt = pd.DataFrame({'id': (X_pred.id).astype(int), 'claim': yhat_bt}, columns=['id', 'claim'])\n\nsubmission_df_lr.to_csv('submissions_Sep21_lr1.csv',index=False)\nsubmission_df_bt.to_csv('submissions_Sep21_bt1.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:55:43.461301Z","iopub.execute_input":"2022-04-14T21:55:43.461829Z","iopub.status.idle":"2022-04-14T21:55:48.453013Z","shell.execute_reply.started":"2022-04-14T21:55:43.461792Z","shell.execute_reply":"2022-04-14T21:55:48.452079Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"os.chdir(r'/kaggle/working')\n\nfrom IPython.display import FileLink\nFileLink(r'submissions_Sep21_lr1.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:55:59.722850Z","iopub.execute_input":"2022-04-14T21:55:59.723127Z","iopub.status.idle":"2022-04-14T21:55:59.729554Z","shell.execute_reply.started":"2022-04-14T21:55:59.723095Z","shell.execute_reply":"2022-04-14T21:55:59.728575Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/submissions_Sep21_lr1.csv","text/html":"<a href='submissions_Sep21_lr1.csv' target='_blank'>submissions_Sep21_lr1.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"# something has gone terribly wrong here. my submissions gte scored as pure random guess...\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T21:39:02.643608Z","iopub.execute_input":"2022-04-14T21:39:02.643873Z","iopub.status.idle":"2022-04-14T21:39:02.658773Z","shell.execute_reply.started":"2022-04-14T21:39:02.643845Z","shell.execute_reply":"2022-04-14T21:39:02.657948Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [id, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17, f18, f19, f20, f21, f22, f23, f24, f25, f26, f27, f28, f29, f30, f31, f32, f33, f34, f35, f36, f37, f38, f39, f40, f41, f42, f43, f44, f45, f46, f47, f48, f49, f50, f51, f52, f53, f54, f55, f56, f57, f58, f59, f60, f61, f62, f63, f64, f65, f66, f67, f68, f69, f70, f71, f72, f73, f74, f75, f76, f77, f78, f79, f80, f81, f82, f83, f84, f85, f86, f87, f88, f89, f90, f91, f92, f93, f94, f95, f96, f97, f98, f99, ...]\nIndex: []\n\n[0 rows x 237 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>...</th>\n      <th>f109_miss</th>\n      <th>f110_miss</th>\n      <th>f111_miss</th>\n      <th>f112_miss</th>\n      <th>f113_miss</th>\n      <th>f114_miss</th>\n      <th>f115_miss</th>\n      <th>f116_miss</th>\n      <th>f117_miss</th>\n      <th>f118_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n<p>0 rows × 237 columns</p>\n</div>"},"metadata":{}}]}]}