{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''Plan:\n1. Load libraries, load data.\n2. Preliminary EDA, dealing with missing values, merging train and test.\n3. EDA, deleting variables.\n4. Feature engineering, ohc.\n5. Sample formation.\n6. Feature scaling.\n7. Model fitting.\n8. Performance evaluation.\n9. Predictions.\n'''\n\n# aside:\n# when coding for interview ML purposes or Kaggle, never drop any obervations!\n# you will have to make predictions for all obs in test sample.\n\n# correct way to deal with missing obs and merge train and test samples:\n# 1. Load both samples.\n# 2. Impute missing values in both samples, using train sample to impute missing values.\n# 3. Concatentate them into df.\n\n\n### 1. Load libraries ###\n\nimport numpy as np\nimport pandas as pd\nimport os, warnings, random, time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\n\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")\n#os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:12:12.249414Z","iopub.execute_input":"2022-04-14T20:12:12.249694Z","iopub.status.idle":"2022-04-14T20:12:13.476485Z","shell.execute_reply.started":"2022-04-14T20:12:12.249662Z","shell.execute_reply":"2022-04-14T20:12:13.475760Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"### Load data ###\n\ntrain = pd.read_csv('../input/tabular-playground-series-sep-2021/train.csv')\nprint(train.shape)\n\ntest = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv')\n\ntrain = train.sample(n=50000)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:12:16.519941Z","iopub.execute_input":"2022-04-14T20:12:16.520499Z","iopub.status.idle":"2022-04-14T20:12:57.361148Z","shell.execute_reply.started":"2022-04-14T20:12:16.520459Z","shell.execute_reply":"2022-04-14T20:12:57.360299Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(957919, 120)\n","output_type":"stream"}]},{"cell_type":"code","source":"### 2. ###\n\ntrain.shape\ntrain.describe()\n\n# are there numerical features?\n\nun_colval = pd.DataFrame([[x,len(train[x].unique())] for x in train.columns], columns = ['colname', 'n_unique'])\nun_colval.loc[un_colval.n_unique < 100]\n# all columns contain numerical features","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:12:58.658105Z","iopub.execute_input":"2022-04-14T20:12:58.658522Z","iopub.status.idle":"2022-04-14T20:12:59.311791Z","shell.execute_reply.started":"2022-04-14T20:12:58.658488Z","shell.execute_reply":"2022-04-14T20:12:59.310944Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"    colname  n_unique\n119   claim         2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>colname</th>\n      <th>n_unique</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>119</th>\n      <td>claim</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# given huge difference in accuracy with and without missing values, i suggest creating dummies for them.\n\n#for i in df.columns:\n#    df['mv'+i] = df[i].isnull()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train.describe()\n#test.describe()\n\n# imputing missing values #\n\ncolmnames = train.columns\ntest['claim'] = np.nan\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\nimp.fit(train)\ntrain = pd.DataFrame(imp.transform(train))\ntest = pd.DataFrame(imp.transform(test))\n#train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:13:02.263330Z","iopub.execute_input":"2022-04-14T20:13:02.263719Z","iopub.status.idle":"2022-04-14T20:13:04.394112Z","shell.execute_reply.started":"2022-04-14T20:13:02.263683Z","shell.execute_reply":"2022-04-14T20:13:04.393331Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train['sample'] = 'train'\ntest['sample'] = 'pred'\ndf = pd.concat([train, test])\n#df.colnames = list(colmnames)+'sample'\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:13:07.277737Z","iopub.execute_input":"2022-04-14T20:13:07.277990Z","iopub.status.idle":"2022-04-14T20:13:07.570977Z","shell.execute_reply.started":"2022-04-14T20:13:07.277959Z","shell.execute_reply":"2022-04-14T20:13:07.570166Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                0         1         2        3         4         5         6  \\\n0        526662.0  0.100030  0.379480  4641.90  0.196980  0.187850  2.405300   \n1        916455.0  0.089585 -0.013797  2675.90  0.182870  0.509330  1.768900   \n2        658605.0  0.083463  0.119050  1235.70  0.003946  0.266620  0.569570   \n3        260818.0  0.102180 -0.001167   538.54  0.167090  0.002191  0.817240   \n4         16585.0  0.103740  0.254490   202.39  0.137390  0.313980 -0.811050   \n...           ...       ...       ...      ...       ...       ...       ...   \n493469  1451388.0 -0.009112  0.308190   637.64  0.778200  0.414150 -1.068500   \n493470  1451389.0  0.088922  0.482650  6924.10  0.025963  0.355400 -0.870200   \n493471  1451390.0  0.140620  0.484750  1797.10  0.147020  0.288030 -1.407100   \n493472  1451391.0  0.168000  0.351760   454.79  0.164580  0.169830  0.323850   \n493473  1451392.0  0.093079  0.501860  3322.40  0.137390  0.332690  0.056827   \n\n              7         8             9  ...     111        112       113  \\\n0       2177.50  547840.0  2.779400e+14  ...  1.7503   37.78100   6.60490   \n1       2106.20  218000.0  9.470900e+15  ...  1.2556   -0.62244   7.58350   \n2       1304.40  175710.0  2.677800e+13  ...  4.0337   -5.52050  -2.56720   \n3        677.15  762580.0  3.677600e+15  ...  2.8960    8.13580  -0.59556   \n4       1151.20  271000.0 -3.164700e+13  ...  3.8834   21.48400  -2.59450   \n...         ...       ...           ...  ...     ...        ...       ...   \n493469   651.22  985000.0  6.079700e+15  ...  2.3325    0.11226  -5.92380   \n493470  2514.20   18004.0  6.073500e+14  ...  1.7005   97.81300   4.37930   \n493471   434.03  333050.0  2.351000e+15  ...  1.6827    1.71750   8.06330   \n493472  2331.20  223980.0 -2.795300e+12  ...  1.3531  155.21000  13.96300   \n493473  2568.70   39185.0  1.489300e+14  ...  1.4480   -1.51290 -15.43600   \n\n             114     115           116      117      118  119  sample  \n0       19338.00  1.1676  6.625100e+16  6816.30  0.40108  0.0   train  \n1       12690.00  1.2952 -1.086200e+14  7623.90  0.89290  0.0   train  \n2        -353.81  1.1646 -1.772300e+15   748.21  0.69200  0.0   train  \n3       94900.00  1.0729  3.363600e+15   347.08  0.31715  1.0   train  \n4       35700.00  1.4914  5.217900e+15  4696.00  0.24021  1.0   train  \n...          ...     ...           ...      ...      ...  ...     ...  \n493469  19338.00  1.1559  8.163900e+16   857.09  1.56330  0.0    pred  \n493470  -2432.00  1.0707  4.691800e+16  7497.10  0.67075  0.0    pred  \n493471   2471.40  1.1725  7.900900e+16  2904.60  0.18005  0.0    pred  \n493472    -11.44  1.1946 -1.770600e+14  6763.10  0.33223  0.0    pred  \n493473    662.46  1.1778  2.459300e+15  8770.00  0.95503  0.0    pred  \n\n[543474 rows x 121 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>111</th>\n      <th>112</th>\n      <th>113</th>\n      <th>114</th>\n      <th>115</th>\n      <th>116</th>\n      <th>117</th>\n      <th>118</th>\n      <th>119</th>\n      <th>sample</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>526662.0</td>\n      <td>0.100030</td>\n      <td>0.379480</td>\n      <td>4641.90</td>\n      <td>0.196980</td>\n      <td>0.187850</td>\n      <td>2.405300</td>\n      <td>2177.50</td>\n      <td>547840.0</td>\n      <td>2.779400e+14</td>\n      <td>...</td>\n      <td>1.7503</td>\n      <td>37.78100</td>\n      <td>6.60490</td>\n      <td>19338.00</td>\n      <td>1.1676</td>\n      <td>6.625100e+16</td>\n      <td>6816.30</td>\n      <td>0.40108</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>916455.0</td>\n      <td>0.089585</td>\n      <td>-0.013797</td>\n      <td>2675.90</td>\n      <td>0.182870</td>\n      <td>0.509330</td>\n      <td>1.768900</td>\n      <td>2106.20</td>\n      <td>218000.0</td>\n      <td>9.470900e+15</td>\n      <td>...</td>\n      <td>1.2556</td>\n      <td>-0.62244</td>\n      <td>7.58350</td>\n      <td>12690.00</td>\n      <td>1.2952</td>\n      <td>-1.086200e+14</td>\n      <td>7623.90</td>\n      <td>0.89290</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>658605.0</td>\n      <td>0.083463</td>\n      <td>0.119050</td>\n      <td>1235.70</td>\n      <td>0.003946</td>\n      <td>0.266620</td>\n      <td>0.569570</td>\n      <td>1304.40</td>\n      <td>175710.0</td>\n      <td>2.677800e+13</td>\n      <td>...</td>\n      <td>4.0337</td>\n      <td>-5.52050</td>\n      <td>-2.56720</td>\n      <td>-353.81</td>\n      <td>1.1646</td>\n      <td>-1.772300e+15</td>\n      <td>748.21</td>\n      <td>0.69200</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>260818.0</td>\n      <td>0.102180</td>\n      <td>-0.001167</td>\n      <td>538.54</td>\n      <td>0.167090</td>\n      <td>0.002191</td>\n      <td>0.817240</td>\n      <td>677.15</td>\n      <td>762580.0</td>\n      <td>3.677600e+15</td>\n      <td>...</td>\n      <td>2.8960</td>\n      <td>8.13580</td>\n      <td>-0.59556</td>\n      <td>94900.00</td>\n      <td>1.0729</td>\n      <td>3.363600e+15</td>\n      <td>347.08</td>\n      <td>0.31715</td>\n      <td>1.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16585.0</td>\n      <td>0.103740</td>\n      <td>0.254490</td>\n      <td>202.39</td>\n      <td>0.137390</td>\n      <td>0.313980</td>\n      <td>-0.811050</td>\n      <td>1151.20</td>\n      <td>271000.0</td>\n      <td>-3.164700e+13</td>\n      <td>...</td>\n      <td>3.8834</td>\n      <td>21.48400</td>\n      <td>-2.59450</td>\n      <td>35700.00</td>\n      <td>1.4914</td>\n      <td>5.217900e+15</td>\n      <td>4696.00</td>\n      <td>0.24021</td>\n      <td>1.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>493469</th>\n      <td>1451388.0</td>\n      <td>-0.009112</td>\n      <td>0.308190</td>\n      <td>637.64</td>\n      <td>0.778200</td>\n      <td>0.414150</td>\n      <td>-1.068500</td>\n      <td>651.22</td>\n      <td>985000.0</td>\n      <td>6.079700e+15</td>\n      <td>...</td>\n      <td>2.3325</td>\n      <td>0.11226</td>\n      <td>-5.92380</td>\n      <td>19338.00</td>\n      <td>1.1559</td>\n      <td>8.163900e+16</td>\n      <td>857.09</td>\n      <td>1.56330</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493470</th>\n      <td>1451389.0</td>\n      <td>0.088922</td>\n      <td>0.482650</td>\n      <td>6924.10</td>\n      <td>0.025963</td>\n      <td>0.355400</td>\n      <td>-0.870200</td>\n      <td>2514.20</td>\n      <td>18004.0</td>\n      <td>6.073500e+14</td>\n      <td>...</td>\n      <td>1.7005</td>\n      <td>97.81300</td>\n      <td>4.37930</td>\n      <td>-2432.00</td>\n      <td>1.0707</td>\n      <td>4.691800e+16</td>\n      <td>7497.10</td>\n      <td>0.67075</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493471</th>\n      <td>1451390.0</td>\n      <td>0.140620</td>\n      <td>0.484750</td>\n      <td>1797.10</td>\n      <td>0.147020</td>\n      <td>0.288030</td>\n      <td>-1.407100</td>\n      <td>434.03</td>\n      <td>333050.0</td>\n      <td>2.351000e+15</td>\n      <td>...</td>\n      <td>1.6827</td>\n      <td>1.71750</td>\n      <td>8.06330</td>\n      <td>2471.40</td>\n      <td>1.1725</td>\n      <td>7.900900e+16</td>\n      <td>2904.60</td>\n      <td>0.18005</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493472</th>\n      <td>1451391.0</td>\n      <td>0.168000</td>\n      <td>0.351760</td>\n      <td>454.79</td>\n      <td>0.164580</td>\n      <td>0.169830</td>\n      <td>0.323850</td>\n      <td>2331.20</td>\n      <td>223980.0</td>\n      <td>-2.795300e+12</td>\n      <td>...</td>\n      <td>1.3531</td>\n      <td>155.21000</td>\n      <td>13.96300</td>\n      <td>-11.44</td>\n      <td>1.1946</td>\n      <td>-1.770600e+14</td>\n      <td>6763.10</td>\n      <td>0.33223</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493473</th>\n      <td>1451392.0</td>\n      <td>0.093079</td>\n      <td>0.501860</td>\n      <td>3322.40</td>\n      <td>0.137390</td>\n      <td>0.332690</td>\n      <td>0.056827</td>\n      <td>2568.70</td>\n      <td>39185.0</td>\n      <td>1.489300e+14</td>\n      <td>...</td>\n      <td>1.4480</td>\n      <td>-1.51290</td>\n      <td>-15.43600</td>\n      <td>662.46</td>\n      <td>1.1778</td>\n      <td>2.459300e+15</td>\n      <td>8770.00</td>\n      <td>0.95503</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n  </tbody>\n</table>\n<p>543474 rows × 121 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"newcolnames = list(colmnames) + ['sample']\ndf.columns = newcolnames\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:13:11.196510Z","iopub.execute_input":"2022-04-14T20:13:11.196761Z","iopub.status.idle":"2022-04-14T20:13:11.313146Z","shell.execute_reply.started":"2022-04-14T20:13:11.196733Z","shell.execute_reply":"2022-04-14T20:13:11.312455Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"               id        f1        f2       f3        f4        f5        f6  \\\n0        526662.0  0.100030  0.379480  4641.90  0.196980  0.187850  2.405300   \n1        916455.0  0.089585 -0.013797  2675.90  0.182870  0.509330  1.768900   \n2        658605.0  0.083463  0.119050  1235.70  0.003946  0.266620  0.569570   \n3        260818.0  0.102180 -0.001167   538.54  0.167090  0.002191  0.817240   \n4         16585.0  0.103740  0.254490   202.39  0.137390  0.313980 -0.811050   \n...           ...       ...       ...      ...       ...       ...       ...   \n493469  1451388.0 -0.009112  0.308190   637.64  0.778200  0.414150 -1.068500   \n493470  1451389.0  0.088922  0.482650  6924.10  0.025963  0.355400 -0.870200   \n493471  1451390.0  0.140620  0.484750  1797.10  0.147020  0.288030 -1.407100   \n493472  1451391.0  0.168000  0.351760   454.79  0.164580  0.169830  0.323850   \n493473  1451392.0  0.093079  0.501860  3322.40  0.137390  0.332690  0.056827   \n\n             f7        f8            f9  ...    f111       f112      f113  \\\n0       2177.50  547840.0  2.779400e+14  ...  1.7503   37.78100   6.60490   \n1       2106.20  218000.0  9.470900e+15  ...  1.2556   -0.62244   7.58350   \n2       1304.40  175710.0  2.677800e+13  ...  4.0337   -5.52050  -2.56720   \n3        677.15  762580.0  3.677600e+15  ...  2.8960    8.13580  -0.59556   \n4       1151.20  271000.0 -3.164700e+13  ...  3.8834   21.48400  -2.59450   \n...         ...       ...           ...  ...     ...        ...       ...   \n493469   651.22  985000.0  6.079700e+15  ...  2.3325    0.11226  -5.92380   \n493470  2514.20   18004.0  6.073500e+14  ...  1.7005   97.81300   4.37930   \n493471   434.03  333050.0  2.351000e+15  ...  1.6827    1.71750   8.06330   \n493472  2331.20  223980.0 -2.795300e+12  ...  1.3531  155.21000  13.96300   \n493473  2568.70   39185.0  1.489300e+14  ...  1.4480   -1.51290 -15.43600   \n\n            f114    f115          f116     f117     f118  claim  sample  \n0       19338.00  1.1676  6.625100e+16  6816.30  0.40108    0.0   train  \n1       12690.00  1.2952 -1.086200e+14  7623.90  0.89290    0.0   train  \n2        -353.81  1.1646 -1.772300e+15   748.21  0.69200    0.0   train  \n3       94900.00  1.0729  3.363600e+15   347.08  0.31715    1.0   train  \n4       35700.00  1.4914  5.217900e+15  4696.00  0.24021    1.0   train  \n...          ...     ...           ...      ...      ...    ...     ...  \n493469  19338.00  1.1559  8.163900e+16   857.09  1.56330    0.0    pred  \n493470  -2432.00  1.0707  4.691800e+16  7497.10  0.67075    0.0    pred  \n493471   2471.40  1.1725  7.900900e+16  2904.60  0.18005    0.0    pred  \n493472    -11.44  1.1946 -1.770600e+14  6763.10  0.33223    0.0    pred  \n493473    662.46  1.1778  2.459300e+15  8770.00  0.95503    0.0    pred  \n\n[543474 rows x 121 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>...</th>\n      <th>f111</th>\n      <th>f112</th>\n      <th>f113</th>\n      <th>f114</th>\n      <th>f115</th>\n      <th>f116</th>\n      <th>f117</th>\n      <th>f118</th>\n      <th>claim</th>\n      <th>sample</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>526662.0</td>\n      <td>0.100030</td>\n      <td>0.379480</td>\n      <td>4641.90</td>\n      <td>0.196980</td>\n      <td>0.187850</td>\n      <td>2.405300</td>\n      <td>2177.50</td>\n      <td>547840.0</td>\n      <td>2.779400e+14</td>\n      <td>...</td>\n      <td>1.7503</td>\n      <td>37.78100</td>\n      <td>6.60490</td>\n      <td>19338.00</td>\n      <td>1.1676</td>\n      <td>6.625100e+16</td>\n      <td>6816.30</td>\n      <td>0.40108</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>916455.0</td>\n      <td>0.089585</td>\n      <td>-0.013797</td>\n      <td>2675.90</td>\n      <td>0.182870</td>\n      <td>0.509330</td>\n      <td>1.768900</td>\n      <td>2106.20</td>\n      <td>218000.0</td>\n      <td>9.470900e+15</td>\n      <td>...</td>\n      <td>1.2556</td>\n      <td>-0.62244</td>\n      <td>7.58350</td>\n      <td>12690.00</td>\n      <td>1.2952</td>\n      <td>-1.086200e+14</td>\n      <td>7623.90</td>\n      <td>0.89290</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>658605.0</td>\n      <td>0.083463</td>\n      <td>0.119050</td>\n      <td>1235.70</td>\n      <td>0.003946</td>\n      <td>0.266620</td>\n      <td>0.569570</td>\n      <td>1304.40</td>\n      <td>175710.0</td>\n      <td>2.677800e+13</td>\n      <td>...</td>\n      <td>4.0337</td>\n      <td>-5.52050</td>\n      <td>-2.56720</td>\n      <td>-353.81</td>\n      <td>1.1646</td>\n      <td>-1.772300e+15</td>\n      <td>748.21</td>\n      <td>0.69200</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>260818.0</td>\n      <td>0.102180</td>\n      <td>-0.001167</td>\n      <td>538.54</td>\n      <td>0.167090</td>\n      <td>0.002191</td>\n      <td>0.817240</td>\n      <td>677.15</td>\n      <td>762580.0</td>\n      <td>3.677600e+15</td>\n      <td>...</td>\n      <td>2.8960</td>\n      <td>8.13580</td>\n      <td>-0.59556</td>\n      <td>94900.00</td>\n      <td>1.0729</td>\n      <td>3.363600e+15</td>\n      <td>347.08</td>\n      <td>0.31715</td>\n      <td>1.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16585.0</td>\n      <td>0.103740</td>\n      <td>0.254490</td>\n      <td>202.39</td>\n      <td>0.137390</td>\n      <td>0.313980</td>\n      <td>-0.811050</td>\n      <td>1151.20</td>\n      <td>271000.0</td>\n      <td>-3.164700e+13</td>\n      <td>...</td>\n      <td>3.8834</td>\n      <td>21.48400</td>\n      <td>-2.59450</td>\n      <td>35700.00</td>\n      <td>1.4914</td>\n      <td>5.217900e+15</td>\n      <td>4696.00</td>\n      <td>0.24021</td>\n      <td>1.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>493469</th>\n      <td>1451388.0</td>\n      <td>-0.009112</td>\n      <td>0.308190</td>\n      <td>637.64</td>\n      <td>0.778200</td>\n      <td>0.414150</td>\n      <td>-1.068500</td>\n      <td>651.22</td>\n      <td>985000.0</td>\n      <td>6.079700e+15</td>\n      <td>...</td>\n      <td>2.3325</td>\n      <td>0.11226</td>\n      <td>-5.92380</td>\n      <td>19338.00</td>\n      <td>1.1559</td>\n      <td>8.163900e+16</td>\n      <td>857.09</td>\n      <td>1.56330</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493470</th>\n      <td>1451389.0</td>\n      <td>0.088922</td>\n      <td>0.482650</td>\n      <td>6924.10</td>\n      <td>0.025963</td>\n      <td>0.355400</td>\n      <td>-0.870200</td>\n      <td>2514.20</td>\n      <td>18004.0</td>\n      <td>6.073500e+14</td>\n      <td>...</td>\n      <td>1.7005</td>\n      <td>97.81300</td>\n      <td>4.37930</td>\n      <td>-2432.00</td>\n      <td>1.0707</td>\n      <td>4.691800e+16</td>\n      <td>7497.10</td>\n      <td>0.67075</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493471</th>\n      <td>1451390.0</td>\n      <td>0.140620</td>\n      <td>0.484750</td>\n      <td>1797.10</td>\n      <td>0.147020</td>\n      <td>0.288030</td>\n      <td>-1.407100</td>\n      <td>434.03</td>\n      <td>333050.0</td>\n      <td>2.351000e+15</td>\n      <td>...</td>\n      <td>1.6827</td>\n      <td>1.71750</td>\n      <td>8.06330</td>\n      <td>2471.40</td>\n      <td>1.1725</td>\n      <td>7.900900e+16</td>\n      <td>2904.60</td>\n      <td>0.18005</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493472</th>\n      <td>1451391.0</td>\n      <td>0.168000</td>\n      <td>0.351760</td>\n      <td>454.79</td>\n      <td>0.164580</td>\n      <td>0.169830</td>\n      <td>0.323850</td>\n      <td>2331.20</td>\n      <td>223980.0</td>\n      <td>-2.795300e+12</td>\n      <td>...</td>\n      <td>1.3531</td>\n      <td>155.21000</td>\n      <td>13.96300</td>\n      <td>-11.44</td>\n      <td>1.1946</td>\n      <td>-1.770600e+14</td>\n      <td>6763.10</td>\n      <td>0.33223</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493473</th>\n      <td>1451392.0</td>\n      <td>0.093079</td>\n      <td>0.501860</td>\n      <td>3322.40</td>\n      <td>0.137390</td>\n      <td>0.332690</td>\n      <td>0.056827</td>\n      <td>2568.70</td>\n      <td>39185.0</td>\n      <td>1.489300e+14</td>\n      <td>...</td>\n      <td>1.4480</td>\n      <td>-1.51290</td>\n      <td>-15.43600</td>\n      <td>662.46</td>\n      <td>1.1778</td>\n      <td>2.459300e+15</td>\n      <td>8770.00</td>\n      <td>0.95503</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n  </tbody>\n</table>\n<p>543474 rows × 121 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"### 3. ###\n\n# we have many variables, are all of them useful? lets see correlation\n\n#cors = df.corr()\n#cors.loc[((cors > 0.8)&(cors<1)).any(1)]\n# there are no pairwise correlations above 80%","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:13:15.416483Z","iopub.execute_input":"2022-04-14T20:13:15.417083Z","iopub.status.idle":"2022-04-14T20:13:15.421433Z","shell.execute_reply.started":"2022-04-14T20:13:15.417021Z","shell.execute_reply":"2022-04-14T20:13:15.419928Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"### 4. ###\n\n# check skew and possibly transform some variables #\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:13:19.267089Z","iopub.execute_input":"2022-04-14T20:13:19.267560Z","iopub.status.idle":"2022-04-14T20:13:19.270907Z","shell.execute_reply.started":"2022-04-14T20:13:19.267523Z","shell.execute_reply":"2022-04-14T20:13:19.270091Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"### 5. ###\n\ny_train = df.loc[df['sample']=='train',['claim']]\nX_train = df.drop(columns=['claim'])\nX_train = X_train.loc[X_train['sample']=='train']\nX_pred = X_train.loc[X_train['sample']=='test']\n\nprint(y_train.shape, X_train.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=2)\n\nprint(y_train.shape, X_train.shape, X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:13:22.612178Z","iopub.execute_input":"2022-04-14T20:13:22.612873Z","iopub.status.idle":"2022-04-14T20:13:22.972320Z","shell.execute_reply.started":"2022-04-14T20:13:22.612831Z","shell.execute_reply":"2022-04-14T20:13:22.971509Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(50000, 1) (50000, 120)\n(45000, 1) (45000, 120) (5000, 120)\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train.drop(columns = ['sample'], inplace=True)\nX_test.drop(columns = ['sample'], inplace=True)\nX_pred.drop(columns = ['sample'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:13:26.472293Z","iopub.execute_input":"2022-04-14T20:13:26.474489Z","iopub.status.idle":"2022-04-14T20:13:26.491477Z","shell.execute_reply.started":"2022-04-14T20:13:26.474452Z","shell.execute_reply":"2022-04-14T20:13:26.490321Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:13:32.860598Z","iopub.execute_input":"2022-04-14T20:13:32.860919Z","iopub.status.idle":"2022-04-14T20:13:32.868903Z","shell.execute_reply.started":"2022-04-14T20:13:32.860884Z","shell.execute_reply":"2022-04-14T20:13:32.868041Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(45000, 119)"},"metadata":{}}]},{"cell_type":"code","source":"### 6. Scaling ###\n\nss = StandardScaler()\n\nfor i in X_train.columns:\n    X_train[[i]] = ss.fit_transform(X_train[[i]])\n    X_test[[i]] = ss.transform(X_test[[i]])\n\nX_train.describe()  ","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:13:35.706000Z","iopub.execute_input":"2022-04-14T20:13:35.706552Z","iopub.status.idle":"2022-04-14T20:13:36.638584Z","shell.execute_reply.started":"2022-04-14T20:13:35.706513Z","shell.execute_reply":"2022-04-14T20:13:36.637882Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                 id            f1            f2            f3            f4  \\\ncount  4.500000e+04  4.500000e+04  4.500000e+04  4.500000e+04  4.500000e+04   \nmean  -3.607176e-17  2.762408e-16 -2.141694e-16  4.891396e-17 -1.479014e-16   \nstd    1.000011e+00  1.000011e+00  1.000011e+00  1.000011e+00  1.000011e+00   \nmin   -1.738263e+00 -5.380555e+00 -2.494517e+00 -1.882106e+00 -1.293609e+00   \n25%   -8.687421e-01 -4.550453e-01 -4.192776e-01 -5.624548e-01 -7.785014e-01   \n50%    1.373289e-02 -9.853223e-04  2.926735e-01 -4.297261e-01 -2.978989e-01   \n75%    8.627608e-01  5.900619e-01  7.633886e-01  3.170292e-02  4.442081e-01   \nmax    1.728839e+00  7.017956e+00  1.167265e+00  5.345938e+00  5.081339e+00   \n\n                 f5            f6            f7            f8            f9  \\\ncount  4.500000e+04  4.500000e+04  4.500000e+04  4.500000e+04  4.500000e+04   \nmean  -4.216726e-16  3.672618e-17 -8.881784e-17  1.320512e-16 -4.470251e-17   \nstd    1.000011e+00  1.000011e+00  1.000011e+00  1.000011e+00  1.000011e+00   \nmin   -2.154454e+00 -5.917176e+00 -1.439784e+00 -1.170910e+00 -1.159069e+00   \n25%   -4.345348e-01 -5.006805e-01 -8.902029e-01 -8.220552e-01 -7.610460e-01   \n50%    1.567628e-01 -1.488625e-01 -1.323340e-01 -2.469617e-01 -5.529532e-01   \n75%    7.379226e-01  4.663880e-01  6.832690e-01  5.178375e-01  5.422382e-01   \nmax    1.662834e+00  5.103952e+00  2.938078e+00  4.421749e+00  3.733693e+00   \n\n       ...          f109          f110          f111          f112  \\\ncount  ...  4.500000e+04  4.500000e+04  4.500000e+04  4.500000e+04   \nmean   ...  1.927347e-16  3.896019e-17 -1.901343e-16 -9.884131e-17   \nstd    ...  1.000011e+00  1.000011e+00  1.000011e+00  1.000011e+00   \nmin    ... -1.450140e+00 -4.493991e+00 -1.862986e+00 -1.006227e+00   \n25%    ... -9.508583e-01 -4.628451e-01 -6.445105e-01 -5.303658e-01   \n50%    ... -1.254630e-01  2.828197e-01 -4.545531e-01 -4.788344e-01   \n75%    ...  7.506944e-01  7.794844e-01  4.337541e-01 -1.120554e-01   \nmax    ...  2.387433e+00  1.140958e+00  2.729458e+00  4.145431e+00   \n\n               f113          f114          f115          f116          f117  \\\ncount  4.500000e+04  4.500000e+04  4.500000e+04  4.500000e+04  4.500000e+04   \nmean  -3.305997e-17  1.244560e-17  3.673111e-17  6.038626e-17  3.010345e-16   \nstd    1.000011e+00  1.000011e+00  1.000011e+00  1.000011e+00  1.000011e+00   \nmin   -2.802943e+00 -1.242087e+00 -2.641919e+00 -7.385613e-01 -1.391548e+00   \n25%   -6.177043e-01 -6.485321e-01 -5.306581e-01 -6.328487e-01 -8.368305e-01   \n50%   -7.860081e-02 -4.679427e-01 -2.680917e-01 -4.349680e-01 -2.257169e-01   \n75%    4.930463e-01  2.484507e-01  2.788489e-01  1.449907e-01  6.894510e-01   \nmax    4.536720e+00  4.830804e+00  5.729936e+00  4.196304e+00  2.945402e+00   \n\n               f118  \ncount  4.500000e+04  \nmean  -1.725521e-16  \nstd    1.000011e+00  \nmin   -1.647440e+00  \n25%   -6.822474e-01  \n50%   -2.073797e-01  \n75%    4.513891e-01  \nmax    5.089434e+00  \n\n[8 rows x 119 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>...</th>\n      <th>f109</th>\n      <th>f110</th>\n      <th>f111</th>\n      <th>f112</th>\n      <th>f113</th>\n      <th>f114</th>\n      <th>f115</th>\n      <th>f116</th>\n      <th>f117</th>\n      <th>f118</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>...</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-3.607176e-17</td>\n      <td>2.762408e-16</td>\n      <td>-2.141694e-16</td>\n      <td>4.891396e-17</td>\n      <td>-1.479014e-16</td>\n      <td>-4.216726e-16</td>\n      <td>3.672618e-17</td>\n      <td>-8.881784e-17</td>\n      <td>1.320512e-16</td>\n      <td>-4.470251e-17</td>\n      <td>...</td>\n      <td>1.927347e-16</td>\n      <td>3.896019e-17</td>\n      <td>-1.901343e-16</td>\n      <td>-9.884131e-17</td>\n      <td>-3.305997e-17</td>\n      <td>1.244560e-17</td>\n      <td>3.673111e-17</td>\n      <td>6.038626e-17</td>\n      <td>3.010345e-16</td>\n      <td>-1.725521e-16</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>...</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.738263e+00</td>\n      <td>-5.380555e+00</td>\n      <td>-2.494517e+00</td>\n      <td>-1.882106e+00</td>\n      <td>-1.293609e+00</td>\n      <td>-2.154454e+00</td>\n      <td>-5.917176e+00</td>\n      <td>-1.439784e+00</td>\n      <td>-1.170910e+00</td>\n      <td>-1.159069e+00</td>\n      <td>...</td>\n      <td>-1.450140e+00</td>\n      <td>-4.493991e+00</td>\n      <td>-1.862986e+00</td>\n      <td>-1.006227e+00</td>\n      <td>-2.802943e+00</td>\n      <td>-1.242087e+00</td>\n      <td>-2.641919e+00</td>\n      <td>-7.385613e-01</td>\n      <td>-1.391548e+00</td>\n      <td>-1.647440e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-8.687421e-01</td>\n      <td>-4.550453e-01</td>\n      <td>-4.192776e-01</td>\n      <td>-5.624548e-01</td>\n      <td>-7.785014e-01</td>\n      <td>-4.345348e-01</td>\n      <td>-5.006805e-01</td>\n      <td>-8.902029e-01</td>\n      <td>-8.220552e-01</td>\n      <td>-7.610460e-01</td>\n      <td>...</td>\n      <td>-9.508583e-01</td>\n      <td>-4.628451e-01</td>\n      <td>-6.445105e-01</td>\n      <td>-5.303658e-01</td>\n      <td>-6.177043e-01</td>\n      <td>-6.485321e-01</td>\n      <td>-5.306581e-01</td>\n      <td>-6.328487e-01</td>\n      <td>-8.368305e-01</td>\n      <td>-6.822474e-01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.373289e-02</td>\n      <td>-9.853223e-04</td>\n      <td>2.926735e-01</td>\n      <td>-4.297261e-01</td>\n      <td>-2.978989e-01</td>\n      <td>1.567628e-01</td>\n      <td>-1.488625e-01</td>\n      <td>-1.323340e-01</td>\n      <td>-2.469617e-01</td>\n      <td>-5.529532e-01</td>\n      <td>...</td>\n      <td>-1.254630e-01</td>\n      <td>2.828197e-01</td>\n      <td>-4.545531e-01</td>\n      <td>-4.788344e-01</td>\n      <td>-7.860081e-02</td>\n      <td>-4.679427e-01</td>\n      <td>-2.680917e-01</td>\n      <td>-4.349680e-01</td>\n      <td>-2.257169e-01</td>\n      <td>-2.073797e-01</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8.627608e-01</td>\n      <td>5.900619e-01</td>\n      <td>7.633886e-01</td>\n      <td>3.170292e-02</td>\n      <td>4.442081e-01</td>\n      <td>7.379226e-01</td>\n      <td>4.663880e-01</td>\n      <td>6.832690e-01</td>\n      <td>5.178375e-01</td>\n      <td>5.422382e-01</td>\n      <td>...</td>\n      <td>7.506944e-01</td>\n      <td>7.794844e-01</td>\n      <td>4.337541e-01</td>\n      <td>-1.120554e-01</td>\n      <td>4.930463e-01</td>\n      <td>2.484507e-01</td>\n      <td>2.788489e-01</td>\n      <td>1.449907e-01</td>\n      <td>6.894510e-01</td>\n      <td>4.513891e-01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.728839e+00</td>\n      <td>7.017956e+00</td>\n      <td>1.167265e+00</td>\n      <td>5.345938e+00</td>\n      <td>5.081339e+00</td>\n      <td>1.662834e+00</td>\n      <td>5.103952e+00</td>\n      <td>2.938078e+00</td>\n      <td>4.421749e+00</td>\n      <td>3.733693e+00</td>\n      <td>...</td>\n      <td>2.387433e+00</td>\n      <td>1.140958e+00</td>\n      <td>2.729458e+00</td>\n      <td>4.145431e+00</td>\n      <td>4.536720e+00</td>\n      <td>4.830804e+00</td>\n      <td>5.729936e+00</td>\n      <td>4.196304e+00</td>\n      <td>2.945402e+00</td>\n      <td>5.089434e+00</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 119 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"### 7. Model fitting ###\n\ntime1 = time.time()\n\nlog_rg = LogisticRegression()\n\ngrid_values = {'penalty': ['l2'], 'C': [0.00001, 0.0001, 0.001, 0.01]}\n\nlr = GridSearchCV(log_rg, param_grid = grid_values, cv=4)\nlr.fit(X_train, y_train)\n\nprint('logistic', lr.best_score_, lr.best_params_, time.time()-time1)\n# after dropping missing I had 86%...","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:13:45.294240Z","iopub.execute_input":"2022-04-14T20:13:45.294762Z","iopub.status.idle":"2022-04-14T20:13:47.836118Z","shell.execute_reply.started":"2022-04-14T20:13:45.294724Z","shell.execute_reply":"2022-04-14T20:13:47.833749Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"logistic 0.5225555555555556 {'C': 1e-05, 'penalty': 'l2'} 2.533942937850952\n","output_type":"stream"}]},{"cell_type":"code","source":"time1 = time.time()\n\nsvm = svm.SVC(kernel='rbf')\n\ngrid_values = {'C':[0.01, 0.1, 1, 5, 10]}\n\nsvm = GridSearchCV(svm, param_grid = grid_values, cv=2)\nsvm.fit(X_train, y_train)\n\nprint('SVM', svm.best_score_, svm.best_params_, time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T19:36:36.268332Z","iopub.execute_input":"2022-04-14T19:36:36.268883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time1 = time.time()\n\nxgbcl = XGBClassifier(tree_method='gpu_hist', gpu_id=0 )\n\ngrid_values = {'n_estimators':[100,200,300],'eta':[0.15, 0.23, 0.3], 'max_depth':[2,3,4]}\n\nxgb = GridSearchCV(xgbcl, param_grid = grid_values, cv=2)\nxgb.fit(X_train, y_train)\n\nprint('XGBoost', xgb.best_score_, xgb.best_params_, time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:32:29.415653Z","iopub.execute_input":"2022-04-14T20:32:29.415932Z","iopub.status.idle":"2022-04-14T20:32:59.726728Z","shell.execute_reply.started":"2022-04-14T20:32:29.415899Z","shell.execute_reply":"2022-04-14T20:32:59.725918Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"[20:32:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:32:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nXGBoost 0.6344000000000001 {'eta': 0.23, 'max_depth': 3, 'n_estimators': 300} 30.303605794906616\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}