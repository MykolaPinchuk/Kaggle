{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''Plan:\n1. Load libraries, load data.\n2. Preliminary EDA, dealing with missing values, merging train and test.\n3. EDA, deleting variables.\n4. Feature engineering, ohc.\n5. Sample formation.\n6. Feature scaling.\n7. Model fitting.\n8. Performance evaluation.\n9. Predictions.\n'''\n\n# aside:\n# when coding for interview ML purposes or Kaggle, never drop any obervations!\n# you will have to make predictions for all obs in test sample.\n\n# correct way to deal with missing obs and merge train and test samples:\n# 1. Load both samples.\n# 2. Impute missing values in both samples, using train sample to impute missing values.\n# 3. Concatentate them into df.\n\n\n### 1. Load libraries ###\n\nimport numpy as np\nimport pandas as pd\nimport os, warnings, random, time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\n\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")\n#os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:47:01.564402Z","iopub.execute_input":"2022-04-14T20:47:01.564736Z","iopub.status.idle":"2022-04-14T20:47:02.822259Z","shell.execute_reply.started":"2022-04-14T20:47:01.564635Z","shell.execute_reply":"2022-04-14T20:47:02.821536Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"### Load data ###\n\ntrain = pd.read_csv('../input/tabular-playground-series-sep-2021/train.csv')\nprint(train.shape)\n\ntest = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv')\n\ntrain = train.sample(n=50000)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:47:06.170610Z","iopub.execute_input":"2022-04-14T20:47:06.171446Z","iopub.status.idle":"2022-04-14T20:47:53.802531Z","shell.execute_reply.started":"2022-04-14T20:47:06.171405Z","shell.execute_reply":"2022-04-14T20:47:53.801708Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(957919, 120)\n","output_type":"stream"}]},{"cell_type":"code","source":"### 2. ###\n\ntrain.shape\ntrain.describe()\n\n# are there numerical features?\n\nun_colval = pd.DataFrame([[x,len(train[x].unique())] for x in train.columns], columns = ['colname', 'n_unique'])\nun_colval.loc[un_colval.n_unique < 100]\n# all columns contain numerical features","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:47:55.597301Z","iopub.execute_input":"2022-04-14T20:47:55.597562Z","iopub.status.idle":"2022-04-14T20:47:56.274641Z","shell.execute_reply.started":"2022-04-14T20:47:55.597535Z","shell.execute_reply":"2022-04-14T20:47:56.273905Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"    colname  n_unique\n119   claim         2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>colname</th>\n      <th>n_unique</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>119</th>\n      <td>claim</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#train0 = train.copy()\n#train = train0.copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:39:28.589689Z","iopub.execute_input":"2022-04-14T20:39:28.589973Z","iopub.status.idle":"2022-04-14T20:39:28.599264Z","shell.execute_reply.started":"2022-04-14T20:39:28.589944Z","shell.execute_reply":"2022-04-14T20:39:28.598393Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# given huge difference in accuracy with and without missing values, i suggest creating dummies for them.\n# to write more general code, will have to create union of mis_cols for train and test.\n\ntrain_mis_cols = [col for col in train.columns if train[col].isnull().any()]\ntest_mis_cols = [col for col in test.columns if test[col].isnull().any()]\n\nfor col in train_mis_cols:\n    train[col + '_miss'] = (train[col].isnull()).astype(int)\n\nfor col in test_mis_cols:\n    test[col + '_miss'] = (test[col].isnull()).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:48:01.661751Z","iopub.execute_input":"2022-04-14T20:48:01.662666Z","iopub.status.idle":"2022-04-14T20:48:02.221795Z","shell.execute_reply.started":"2022-04-14T20:48:01.662587Z","shell.execute_reply":"2022-04-14T20:48:02.220944Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:41:46.466675Z","iopub.execute_input":"2022-04-14T20:41:46.466919Z","iopub.status.idle":"2022-04-14T20:41:46.501401Z","shell.execute_reply.started":"2022-04-14T20:41:46.466895Z","shell.execute_reply":"2022-04-14T20:41:46.500632Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"            id        f1       f2         f3        f4       f5       f6  \\\n682454  682454  0.098222  0.49814   1962.400  0.039480  0.50761 -1.48620   \n775441  775441  0.100890  0.43360   4898.800  0.006724  0.31073 -0.65103   \n349573  349573  0.123720  0.48111   1584.700  0.006001  0.17354  1.69800   \n577077  577077  0.100010  0.47666    902.080  0.052631  0.34145  0.99562   \n835145  835145  0.099639      NaN   1632.300  0.403470  0.46886  0.32978   \n...        ...       ...      ...        ...       ...      ...      ...   \n815374  815374  0.055024  0.48326   7928.500  0.046148  0.44481 -3.53350   \n102276  102276  0.101200  0.48012    244.490  0.112510  0.34945 -2.98280   \n94927    94927  0.128640  0.35725     59.309  0.149790  0.27945  0.47255   \n799164  799164  0.156920  0.48848  10637.000  0.020328  0.21993 -1.63840   \n921936  921936  0.005042  0.42917    720.180  0.002701  0.25868  1.35830   \n\n              f7        f8            f9  ...  f109_miss  f110_miss  \\\n682454   336.430   49739.0  4.866100e+15  ...          0          0   \n775441  3795.500  795450.0  1.455800e+14  ...          0          0   \n349573  1762.200  213400.0  7.848400e+15  ...          0          0   \n577077  2371.900  186950.0  3.232100e+15  ...          0          0   \n835145  2000.700  274220.0  2.103800e+15  ...          0          0   \n...          ...       ...           ...  ...        ...        ...   \n815374   -22.124  321040.0  6.053300e+15  ...          0          0   \n102276  2505.600  418060.0  8.072800e+13  ...          0          0   \n94927   2655.200   37296.0 -4.003500e+12  ...          0          0   \n799164   131.860  212000.0  1.815800e+15  ...          0          0   \n921936    35.940   80660.0  3.690300e+13  ...          0          0   \n\n        f111_miss  f112_miss  f113_miss  f114_miss  f115_miss  f116_miss  \\\n682454          0          0          0          0          0          0   \n775441          0          0          0          0          0          0   \n349573          0          0          0          0          0          0   \n577077          0          0          0          0          0          0   \n835145          0          0          0          0          0          0   \n...           ...        ...        ...        ...        ...        ...   \n815374          0          0          0          1          0          0   \n102276          0          0          0          0          0          0   \n94927           0          0          0          0          0          0   \n799164          0          0          0          0          0          0   \n921936          0          0          0          0          0          0   \n\n        f117_miss  f118_miss  \n682454          0          0  \n775441          0          0  \n349573          0          0  \n577077          0          0  \n835145          0          0  \n...           ...        ...  \n815374          0          0  \n102276          0          0  \n94927           0          0  \n799164          0          0  \n921936          0          0  \n\n[50000 rows x 238 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>...</th>\n      <th>f109_miss</th>\n      <th>f110_miss</th>\n      <th>f111_miss</th>\n      <th>f112_miss</th>\n      <th>f113_miss</th>\n      <th>f114_miss</th>\n      <th>f115_miss</th>\n      <th>f116_miss</th>\n      <th>f117_miss</th>\n      <th>f118_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>682454</th>\n      <td>682454</td>\n      <td>0.098222</td>\n      <td>0.49814</td>\n      <td>1962.400</td>\n      <td>0.039480</td>\n      <td>0.50761</td>\n      <td>-1.48620</td>\n      <td>336.430</td>\n      <td>49739.0</td>\n      <td>4.866100e+15</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>775441</th>\n      <td>775441</td>\n      <td>0.100890</td>\n      <td>0.43360</td>\n      <td>4898.800</td>\n      <td>0.006724</td>\n      <td>0.31073</td>\n      <td>-0.65103</td>\n      <td>3795.500</td>\n      <td>795450.0</td>\n      <td>1.455800e+14</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>349573</th>\n      <td>349573</td>\n      <td>0.123720</td>\n      <td>0.48111</td>\n      <td>1584.700</td>\n      <td>0.006001</td>\n      <td>0.17354</td>\n      <td>1.69800</td>\n      <td>1762.200</td>\n      <td>213400.0</td>\n      <td>7.848400e+15</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>577077</th>\n      <td>577077</td>\n      <td>0.100010</td>\n      <td>0.47666</td>\n      <td>902.080</td>\n      <td>0.052631</td>\n      <td>0.34145</td>\n      <td>0.99562</td>\n      <td>2371.900</td>\n      <td>186950.0</td>\n      <td>3.232100e+15</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>835145</th>\n      <td>835145</td>\n      <td>0.099639</td>\n      <td>NaN</td>\n      <td>1632.300</td>\n      <td>0.403470</td>\n      <td>0.46886</td>\n      <td>0.32978</td>\n      <td>2000.700</td>\n      <td>274220.0</td>\n      <td>2.103800e+15</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>815374</th>\n      <td>815374</td>\n      <td>0.055024</td>\n      <td>0.48326</td>\n      <td>7928.500</td>\n      <td>0.046148</td>\n      <td>0.44481</td>\n      <td>-3.53350</td>\n      <td>-22.124</td>\n      <td>321040.0</td>\n      <td>6.053300e+15</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>102276</th>\n      <td>102276</td>\n      <td>0.101200</td>\n      <td>0.48012</td>\n      <td>244.490</td>\n      <td>0.112510</td>\n      <td>0.34945</td>\n      <td>-2.98280</td>\n      <td>2505.600</td>\n      <td>418060.0</td>\n      <td>8.072800e+13</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>94927</th>\n      <td>94927</td>\n      <td>0.128640</td>\n      <td>0.35725</td>\n      <td>59.309</td>\n      <td>0.149790</td>\n      <td>0.27945</td>\n      <td>0.47255</td>\n      <td>2655.200</td>\n      <td>37296.0</td>\n      <td>-4.003500e+12</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>799164</th>\n      <td>799164</td>\n      <td>0.156920</td>\n      <td>0.48848</td>\n      <td>10637.000</td>\n      <td>0.020328</td>\n      <td>0.21993</td>\n      <td>-1.63840</td>\n      <td>131.860</td>\n      <td>212000.0</td>\n      <td>1.815800e+15</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>921936</th>\n      <td>921936</td>\n      <td>0.005042</td>\n      <td>0.42917</td>\n      <td>720.180</td>\n      <td>0.002701</td>\n      <td>0.25868</td>\n      <td>1.35830</td>\n      <td>35.940</td>\n      <td>80660.0</td>\n      <td>3.690300e+13</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 238 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#train.describe()\n#test.describe()\n\n# imputing missing values #\n\ncolmnames = train.columns\ntest['claim'] = np.nan\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\nimp.fit(train)\ntrain = pd.DataFrame(imp.transform(train))\ntest = pd.DataFrame(imp.transform(test))\n#train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:48:12.255081Z","iopub.execute_input":"2022-04-14T20:48:12.255737Z","iopub.status.idle":"2022-04-14T20:48:16.047181Z","shell.execute_reply.started":"2022-04-14T20:48:12.255696Z","shell.execute_reply":"2022-04-14T20:48:16.046439Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train['sample'] = 'train'\ntest['sample'] = 'pred'\ndf = pd.concat([train, test])\n#df.colnames = list(colmnames)+'sample'\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:48:18.372293Z","iopub.execute_input":"2022-04-14T20:48:18.373008Z","iopub.status.idle":"2022-04-14T20:48:18.838040Z","shell.execute_reply.started":"2022-04-14T20:48:18.372971Z","shell.execute_reply":"2022-04-14T20:48:18.837099Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                0         1         2         3         4        5         6  \\\n0        729538.0  0.131690  0.444760   2617.70  0.004542  0.28646 -0.566790   \n1        575499.0  0.130780  0.419920   5683.20  0.227520  0.46524  0.491870   \n2        662663.0  0.101560  0.002789   2393.60  0.301370  0.18251 -1.746000   \n3         95680.0  0.030495  0.514900   6877.50  0.110840  0.32847 -1.876900   \n4        454978.0  0.058061  0.485070  11964.00  0.330550  0.38319 -1.407400   \n...           ...       ...       ...       ...       ...      ...       ...   \n493469  1451388.0 -0.009112  0.308190    637.64  0.778200  0.41415 -1.068500   \n493470  1451389.0  0.088922  0.482650   6924.10  0.025963  0.35540 -0.870200   \n493471  1451390.0  0.140620  0.484750   1797.10  0.147020  0.28803 -1.407100   \n493472  1451391.0  0.168000  0.351760    454.79  0.164580  0.16983  0.323850   \n493473  1451392.0  0.093079  0.501860   3322.40  0.137350  0.33269  0.056827   \n\n              7         8             9  ...  229  230  231  232  233  234  \\\n0         40.83   46886.0 -6.484500e+12  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n1        341.60  587020.0  1.747700e+15  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n2       1439.40  418770.0  6.674500e+14  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n3       1784.40  162930.0  3.070700e+14  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n4       2073.80   56285.0  7.408900e+15  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...       ...           ...  ...  ...  ...  ...  ...  ...  ...   \n493469   651.22  985000.0  6.079700e+15  ...  0.0  0.0  0.0  1.0  0.0  0.0   \n493470  2514.20   18004.0  6.073500e+14  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n493471   434.03  333050.0  2.351000e+15  ...  0.0  1.0  0.0  0.0  0.0  0.0   \n493472  2331.20  223980.0 -2.795300e+12  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n493473  2568.70   39185.0  1.489300e+14  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n\n        235  236  237  sample  \n0       0.0  0.0  0.0   train  \n1       0.0  0.0  0.0   train  \n2       0.0  0.0  0.0   train  \n3       0.0  0.0  0.0   train  \n4       0.0  0.0  0.0   train  \n...     ...  ...  ...     ...  \n493469  0.0  0.0  0.0    pred  \n493470  0.0  0.0  0.0    pred  \n493471  0.0  0.0  0.0    pred  \n493472  0.0  0.0  0.0    pred  \n493473  0.0  0.0  0.0    pred  \n\n[543474 rows x 239 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>229</th>\n      <th>230</th>\n      <th>231</th>\n      <th>232</th>\n      <th>233</th>\n      <th>234</th>\n      <th>235</th>\n      <th>236</th>\n      <th>237</th>\n      <th>sample</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>729538.0</td>\n      <td>0.131690</td>\n      <td>0.444760</td>\n      <td>2617.70</td>\n      <td>0.004542</td>\n      <td>0.28646</td>\n      <td>-0.566790</td>\n      <td>40.83</td>\n      <td>46886.0</td>\n      <td>-6.484500e+12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>575499.0</td>\n      <td>0.130780</td>\n      <td>0.419920</td>\n      <td>5683.20</td>\n      <td>0.227520</td>\n      <td>0.46524</td>\n      <td>0.491870</td>\n      <td>341.60</td>\n      <td>587020.0</td>\n      <td>1.747700e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>662663.0</td>\n      <td>0.101560</td>\n      <td>0.002789</td>\n      <td>2393.60</td>\n      <td>0.301370</td>\n      <td>0.18251</td>\n      <td>-1.746000</td>\n      <td>1439.40</td>\n      <td>418770.0</td>\n      <td>6.674500e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>95680.0</td>\n      <td>0.030495</td>\n      <td>0.514900</td>\n      <td>6877.50</td>\n      <td>0.110840</td>\n      <td>0.32847</td>\n      <td>-1.876900</td>\n      <td>1784.40</td>\n      <td>162930.0</td>\n      <td>3.070700e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>454978.0</td>\n      <td>0.058061</td>\n      <td>0.485070</td>\n      <td>11964.00</td>\n      <td>0.330550</td>\n      <td>0.38319</td>\n      <td>-1.407400</td>\n      <td>2073.80</td>\n      <td>56285.0</td>\n      <td>7.408900e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>493469</th>\n      <td>1451388.0</td>\n      <td>-0.009112</td>\n      <td>0.308190</td>\n      <td>637.64</td>\n      <td>0.778200</td>\n      <td>0.41415</td>\n      <td>-1.068500</td>\n      <td>651.22</td>\n      <td>985000.0</td>\n      <td>6.079700e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493470</th>\n      <td>1451389.0</td>\n      <td>0.088922</td>\n      <td>0.482650</td>\n      <td>6924.10</td>\n      <td>0.025963</td>\n      <td>0.35540</td>\n      <td>-0.870200</td>\n      <td>2514.20</td>\n      <td>18004.0</td>\n      <td>6.073500e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493471</th>\n      <td>1451390.0</td>\n      <td>0.140620</td>\n      <td>0.484750</td>\n      <td>1797.10</td>\n      <td>0.147020</td>\n      <td>0.28803</td>\n      <td>-1.407100</td>\n      <td>434.03</td>\n      <td>333050.0</td>\n      <td>2.351000e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493472</th>\n      <td>1451391.0</td>\n      <td>0.168000</td>\n      <td>0.351760</td>\n      <td>454.79</td>\n      <td>0.164580</td>\n      <td>0.16983</td>\n      <td>0.323850</td>\n      <td>2331.20</td>\n      <td>223980.0</td>\n      <td>-2.795300e+12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493473</th>\n      <td>1451392.0</td>\n      <td>0.093079</td>\n      <td>0.501860</td>\n      <td>3322.40</td>\n      <td>0.137350</td>\n      <td>0.33269</td>\n      <td>0.056827</td>\n      <td>2568.70</td>\n      <td>39185.0</td>\n      <td>1.489300e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n  </tbody>\n</table>\n<p>543474 rows × 239 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"newcolnames = list(colmnames) + ['sample']\ndf.columns = newcolnames\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:48:23.843074Z","iopub.execute_input":"2022-04-14T20:48:23.843809Z","iopub.status.idle":"2022-04-14T20:48:23.963765Z","shell.execute_reply.started":"2022-04-14T20:48:23.843772Z","shell.execute_reply":"2022-04-14T20:48:23.963021Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"               id        f1        f2        f3        f4       f5        f6  \\\n0        729538.0  0.131690  0.444760   2617.70  0.004542  0.28646 -0.566790   \n1        575499.0  0.130780  0.419920   5683.20  0.227520  0.46524  0.491870   \n2        662663.0  0.101560  0.002789   2393.60  0.301370  0.18251 -1.746000   \n3         95680.0  0.030495  0.514900   6877.50  0.110840  0.32847 -1.876900   \n4        454978.0  0.058061  0.485070  11964.00  0.330550  0.38319 -1.407400   \n...           ...       ...       ...       ...       ...      ...       ...   \n493469  1451388.0 -0.009112  0.308190    637.64  0.778200  0.41415 -1.068500   \n493470  1451389.0  0.088922  0.482650   6924.10  0.025963  0.35540 -0.870200   \n493471  1451390.0  0.140620  0.484750   1797.10  0.147020  0.28803 -1.407100   \n493472  1451391.0  0.168000  0.351760    454.79  0.164580  0.16983  0.323850   \n493473  1451392.0  0.093079  0.501860   3322.40  0.137350  0.33269  0.056827   \n\n             f7        f8            f9  ...  f110_miss  f111_miss  f112_miss  \\\n0         40.83   46886.0 -6.484500e+12  ...        0.0        0.0        0.0   \n1        341.60  587020.0  1.747700e+15  ...        0.0        0.0        0.0   \n2       1439.40  418770.0  6.674500e+14  ...        0.0        0.0        0.0   \n3       1784.40  162930.0  3.070700e+14  ...        0.0        0.0        0.0   \n4       2073.80   56285.0  7.408900e+15  ...        0.0        0.0        0.0   \n...         ...       ...           ...  ...        ...        ...        ...   \n493469   651.22  985000.0  6.079700e+15  ...        0.0        0.0        0.0   \n493470  2514.20   18004.0  6.073500e+14  ...        0.0        0.0        0.0   \n493471   434.03  333050.0  2.351000e+15  ...        0.0        1.0        0.0   \n493472  2331.20  223980.0 -2.795300e+12  ...        0.0        0.0        0.0   \n493473  2568.70   39185.0  1.489300e+14  ...        0.0        0.0        0.0   \n\n        f113_miss  f114_miss  f115_miss  f116_miss  f117_miss  f118_miss  \\\n0             0.0        0.0        0.0        0.0        0.0        0.0   \n1             0.0        0.0        0.0        0.0        0.0        0.0   \n2             0.0        0.0        0.0        0.0        0.0        0.0   \n3             0.0        0.0        0.0        0.0        0.0        0.0   \n4             0.0        0.0        0.0        0.0        0.0        0.0   \n...           ...        ...        ...        ...        ...        ...   \n493469        1.0        0.0        0.0        0.0        0.0        0.0   \n493470        0.0        0.0        0.0        0.0        0.0        0.0   \n493471        0.0        0.0        0.0        0.0        0.0        0.0   \n493472        0.0        0.0        0.0        0.0        0.0        0.0   \n493473        0.0        0.0        0.0        0.0        0.0        0.0   \n\n        sample  \n0        train  \n1        train  \n2        train  \n3        train  \n4        train  \n...        ...  \n493469    pred  \n493470    pred  \n493471    pred  \n493472    pred  \n493473    pred  \n\n[543474 rows x 239 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>...</th>\n      <th>f110_miss</th>\n      <th>f111_miss</th>\n      <th>f112_miss</th>\n      <th>f113_miss</th>\n      <th>f114_miss</th>\n      <th>f115_miss</th>\n      <th>f116_miss</th>\n      <th>f117_miss</th>\n      <th>f118_miss</th>\n      <th>sample</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>729538.0</td>\n      <td>0.131690</td>\n      <td>0.444760</td>\n      <td>2617.70</td>\n      <td>0.004542</td>\n      <td>0.28646</td>\n      <td>-0.566790</td>\n      <td>40.83</td>\n      <td>46886.0</td>\n      <td>-6.484500e+12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>575499.0</td>\n      <td>0.130780</td>\n      <td>0.419920</td>\n      <td>5683.20</td>\n      <td>0.227520</td>\n      <td>0.46524</td>\n      <td>0.491870</td>\n      <td>341.60</td>\n      <td>587020.0</td>\n      <td>1.747700e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>662663.0</td>\n      <td>0.101560</td>\n      <td>0.002789</td>\n      <td>2393.60</td>\n      <td>0.301370</td>\n      <td>0.18251</td>\n      <td>-1.746000</td>\n      <td>1439.40</td>\n      <td>418770.0</td>\n      <td>6.674500e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>95680.0</td>\n      <td>0.030495</td>\n      <td>0.514900</td>\n      <td>6877.50</td>\n      <td>0.110840</td>\n      <td>0.32847</td>\n      <td>-1.876900</td>\n      <td>1784.40</td>\n      <td>162930.0</td>\n      <td>3.070700e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>454978.0</td>\n      <td>0.058061</td>\n      <td>0.485070</td>\n      <td>11964.00</td>\n      <td>0.330550</td>\n      <td>0.38319</td>\n      <td>-1.407400</td>\n      <td>2073.80</td>\n      <td>56285.0</td>\n      <td>7.408900e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>493469</th>\n      <td>1451388.0</td>\n      <td>-0.009112</td>\n      <td>0.308190</td>\n      <td>637.64</td>\n      <td>0.778200</td>\n      <td>0.41415</td>\n      <td>-1.068500</td>\n      <td>651.22</td>\n      <td>985000.0</td>\n      <td>6.079700e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493470</th>\n      <td>1451389.0</td>\n      <td>0.088922</td>\n      <td>0.482650</td>\n      <td>6924.10</td>\n      <td>0.025963</td>\n      <td>0.35540</td>\n      <td>-0.870200</td>\n      <td>2514.20</td>\n      <td>18004.0</td>\n      <td>6.073500e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493471</th>\n      <td>1451390.0</td>\n      <td>0.140620</td>\n      <td>0.484750</td>\n      <td>1797.10</td>\n      <td>0.147020</td>\n      <td>0.28803</td>\n      <td>-1.407100</td>\n      <td>434.03</td>\n      <td>333050.0</td>\n      <td>2.351000e+15</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493472</th>\n      <td>1451391.0</td>\n      <td>0.168000</td>\n      <td>0.351760</td>\n      <td>454.79</td>\n      <td>0.164580</td>\n      <td>0.16983</td>\n      <td>0.323850</td>\n      <td>2331.20</td>\n      <td>223980.0</td>\n      <td>-2.795300e+12</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n    <tr>\n      <th>493473</th>\n      <td>1451392.0</td>\n      <td>0.093079</td>\n      <td>0.501860</td>\n      <td>3322.40</td>\n      <td>0.137350</td>\n      <td>0.33269</td>\n      <td>0.056827</td>\n      <td>2568.70</td>\n      <td>39185.0</td>\n      <td>1.489300e+14</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>pred</td>\n    </tr>\n  </tbody>\n</table>\n<p>543474 rows × 239 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"### 3. ###\n\n# we have many variables, are all of them useful? lets see correlation\n\n#cors = df.corr()\n#cors.loc[((cors > 0.8)&(cors<1)).any(1)]\n# there are no pairwise correlations above 80%","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:48:27.380524Z","iopub.execute_input":"2022-04-14T20:48:27.380776Z","iopub.status.idle":"2022-04-14T20:48:27.384656Z","shell.execute_reply.started":"2022-04-14T20:48:27.380749Z","shell.execute_reply":"2022-04-14T20:48:27.383946Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"### 4. ###\n\n# check skew and possibly transform some variables #\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:48:28.730617Z","iopub.execute_input":"2022-04-14T20:48:28.731242Z","iopub.status.idle":"2022-04-14T20:48:28.734285Z","shell.execute_reply.started":"2022-04-14T20:48:28.731185Z","shell.execute_reply":"2022-04-14T20:48:28.733624Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"### 5. ###\n\ny_train = df.loc[df['sample']=='train',['claim']]\nX_train = df.drop(columns=['claim'])\nX_train = X_train.loc[X_train['sample']=='train']\nX_pred = X_train.loc[X_train['sample']=='test']\n\nprint(y_train.shape, X_train.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=2)\n\nprint(y_train.shape, X_train.shape, X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:48:30.435249Z","iopub.execute_input":"2022-04-14T20:48:30.435773Z","iopub.status.idle":"2022-04-14T20:48:30.985959Z","shell.execute_reply.started":"2022-04-14T20:48:30.435736Z","shell.execute_reply":"2022-04-14T20:48:30.984954Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(50000, 1) (50000, 238)\n(45000, 1) (45000, 238) (5000, 238)\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train.drop(columns = ['sample'], inplace=True)\nX_test.drop(columns = ['sample'], inplace=True)\nX_pred.drop(columns = ['sample'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:48:34.196727Z","iopub.execute_input":"2022-04-14T20:48:34.197654Z","iopub.status.idle":"2022-04-14T20:48:34.227228Z","shell.execute_reply.started":"2022-04-14T20:48:34.197601Z","shell.execute_reply":"2022-04-14T20:48:34.226471Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"X_train.columns[0:119]","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:44:38.123535Z","iopub.execute_input":"2022-04-14T20:44:38.124157Z","iopub.status.idle":"2022-04-14T20:44:38.131503Z","shell.execute_reply.started":"2022-04-14T20:44:38.124124Z","shell.execute_reply":"2022-04-14T20:44:38.130677Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Index(['id', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9',\n       ...\n       'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117',\n       'f118'],\n      dtype='object', length=119)"},"metadata":{}}]},{"cell_type":"code","source":"### 6. Scaling ###\n\nss = StandardScaler()\n\nfor i in X_train.columns[0:119]:\n    X_train[[i]] = ss.fit_transform(X_train[[i]])\n    X_test[[i]] = ss.transform(X_test[[i]])\n\nX_train.describe()  ","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:48:38.316951Z","iopub.execute_input":"2022-04-14T20:48:38.317246Z","iopub.status.idle":"2022-04-14T20:48:39.637535Z","shell.execute_reply.started":"2022-04-14T20:48:38.317193Z","shell.execute_reply":"2022-04-14T20:48:39.636695Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                 id            f1            f2            f3            f4  \\\ncount  4.500000e+04  4.500000e+04  4.500000e+04  4.500000e+04  4.500000e+04   \nmean   8.706122e-17  3.066165e-16  8.147557e-17 -9.146264e-17 -1.037343e-16   \nstd    1.000011e+00  1.000011e+00  1.000011e+00  1.000011e+00  1.000011e+00   \nmin   -1.734529e+00 -5.421682e+00 -2.518096e+00 -2.078263e+00 -1.331297e+00   \n25%   -8.699589e-01 -4.577459e-01 -4.157796e-01 -5.673929e-01 -7.726334e-01   \n50%    6.465769e-03 -7.762291e-03  2.892447e-01 -4.307851e-01 -3.034138e-01   \n75%    8.597882e-01  5.904224e-01  7.613421e-01  6.351786e-02  4.398127e-01   \nmax    1.731024e+00  7.152474e+00  1.173556e+00  5.511195e+00  5.057952e+00   \n\n                 f5            f6            f7            f8            f9  \\\ncount  4.500000e+04  4.500000e+04  4.500000e+04  4.500000e+04  4.500000e+04   \nmean   3.078525e-16  2.129161e-18  9.505483e-17 -1.262743e-16 -4.480367e-17   \nstd    1.000011e+00  1.000011e+00  1.000011e+00  1.000011e+00  1.000011e+00   \nmin   -2.168099e+00 -5.595988e+00 -1.442557e+00 -1.172805e+00 -1.225903e+00   \n25%   -4.358995e-01 -4.952981e-01 -8.850075e-01 -8.243322e-01 -7.650833e-01   \n50%    1.526149e-01 -1.505378e-01 -1.442736e-01 -2.559475e-01 -5.505530e-01   \n75%    7.379532e-01  4.646879e-01  6.780546e-01  5.211848e-01  5.540387e-01   \nmax    1.665928e+00  5.224056e+00  2.975679e+00  4.284726e+00  3.637734e+00   \n\n       ...     f109_miss     f110_miss     f111_miss     f112_miss  \\\ncount  ...  45000.000000  45000.000000  45000.000000  45000.000000   \nmean   ...      0.016378      0.014933      0.015622      0.016867   \nstd    ...      0.126925      0.121287      0.124010      0.128773   \nmin    ...      0.000000      0.000000      0.000000      0.000000   \n25%    ...      0.000000      0.000000      0.000000      0.000000   \n50%    ...      0.000000      0.000000      0.000000      0.000000   \n75%    ...      0.000000      0.000000      0.000000      0.000000   \nmax    ...      1.000000      1.000000      1.000000      1.000000   \n\n          f113_miss     f114_miss     f115_miss     f116_miss     f117_miss  \\\ncount  45000.000000  45000.000000  45000.000000  45000.000000  45000.000000   \nmean       0.017600      0.016511      0.017044      0.016622      0.016933   \nstd        0.131494      0.127432      0.129438      0.127853      0.129023   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n          f118_miss  \ncount  45000.000000  \nmean       0.016800  \nstd        0.128523  \nmin        0.000000  \n25%        0.000000  \n50%        0.000000  \n75%        0.000000  \nmax        1.000000  \n\n[8 rows x 237 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>...</th>\n      <th>f109_miss</th>\n      <th>f110_miss</th>\n      <th>f111_miss</th>\n      <th>f112_miss</th>\n      <th>f113_miss</th>\n      <th>f114_miss</th>\n      <th>f115_miss</th>\n      <th>f116_miss</th>\n      <th>f117_miss</th>\n      <th>f118_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>4.500000e+04</td>\n      <td>...</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n      <td>45000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>8.706122e-17</td>\n      <td>3.066165e-16</td>\n      <td>8.147557e-17</td>\n      <td>-9.146264e-17</td>\n      <td>-1.037343e-16</td>\n      <td>3.078525e-16</td>\n      <td>2.129161e-18</td>\n      <td>9.505483e-17</td>\n      <td>-1.262743e-16</td>\n      <td>-4.480367e-17</td>\n      <td>...</td>\n      <td>0.016378</td>\n      <td>0.014933</td>\n      <td>0.015622</td>\n      <td>0.016867</td>\n      <td>0.017600</td>\n      <td>0.016511</td>\n      <td>0.017044</td>\n      <td>0.016622</td>\n      <td>0.016933</td>\n      <td>0.016800</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>1.000011e+00</td>\n      <td>...</td>\n      <td>0.126925</td>\n      <td>0.121287</td>\n      <td>0.124010</td>\n      <td>0.128773</td>\n      <td>0.131494</td>\n      <td>0.127432</td>\n      <td>0.129438</td>\n      <td>0.127853</td>\n      <td>0.129023</td>\n      <td>0.128523</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.734529e+00</td>\n      <td>-5.421682e+00</td>\n      <td>-2.518096e+00</td>\n      <td>-2.078263e+00</td>\n      <td>-1.331297e+00</td>\n      <td>-2.168099e+00</td>\n      <td>-5.595988e+00</td>\n      <td>-1.442557e+00</td>\n      <td>-1.172805e+00</td>\n      <td>-1.225903e+00</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-8.699589e-01</td>\n      <td>-4.577459e-01</td>\n      <td>-4.157796e-01</td>\n      <td>-5.673929e-01</td>\n      <td>-7.726334e-01</td>\n      <td>-4.358995e-01</td>\n      <td>-4.952981e-01</td>\n      <td>-8.850075e-01</td>\n      <td>-8.243322e-01</td>\n      <td>-7.650833e-01</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>6.465769e-03</td>\n      <td>-7.762291e-03</td>\n      <td>2.892447e-01</td>\n      <td>-4.307851e-01</td>\n      <td>-3.034138e-01</td>\n      <td>1.526149e-01</td>\n      <td>-1.505378e-01</td>\n      <td>-1.442736e-01</td>\n      <td>-2.559475e-01</td>\n      <td>-5.505530e-01</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8.597882e-01</td>\n      <td>5.904224e-01</td>\n      <td>7.613421e-01</td>\n      <td>6.351786e-02</td>\n      <td>4.398127e-01</td>\n      <td>7.379532e-01</td>\n      <td>4.646879e-01</td>\n      <td>6.780546e-01</td>\n      <td>5.211848e-01</td>\n      <td>5.540387e-01</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.731024e+00</td>\n      <td>7.152474e+00</td>\n      <td>1.173556e+00</td>\n      <td>5.511195e+00</td>\n      <td>5.057952e+00</td>\n      <td>1.665928e+00</td>\n      <td>5.224056e+00</td>\n      <td>2.975679e+00</td>\n      <td>4.284726e+00</td>\n      <td>3.637734e+00</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 237 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"### 7. Model fitting ###\n\ntime1 = time.time()\n\nlog_rg = LogisticRegression()\n\ngrid_values = {'penalty': ['l2'], 'C': [0.01, 0.1, 0.3, 1, 3, 10]}\n\nlr = GridSearchCV(log_rg, param_grid = grid_values, cv=4)\nlr.fit(X_train, y_train)\n\nprint('logistic', lr.best_score_, lr.best_params_, time.time()-time1)\n# after dropping missing I had 86%. while imputing them with median, i had 52%. with miss dummies, 73%.","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:50:20.691129Z","iopub.execute_input":"2022-04-14T20:50:20.691831Z","iopub.status.idle":"2022-04-14T20:50:35.455072Z","shell.execute_reply.started":"2022-04-14T20:50:20.691792Z","shell.execute_reply":"2022-04-14T20:50:35.451549Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"logistic 0.7273777777777778 {'C': 1, 'penalty': 'l2'} 14.74954080581665\n","output_type":"stream"}]},{"cell_type":"code","source":"time1 = time.time()\n\nsvm = svm.SVC(kernel='rbf')\n\ngrid_values = {'C':[0.01, 0.1, 1, 5, 10]}\n\nsvm = GridSearchCV(svm, param_grid = grid_values, cv=2)\nsvm.fit(X_train, y_train)\n\nprint('SVM', svm.best_score_, svm.best_params_, time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T19:36:36.268332Z","iopub.execute_input":"2022-04-14T19:36:36.268883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time1 = time.time()\n\nxgbcl = XGBClassifier(tree_method='gpu_hist', gpu_id=0 )\n\ngrid_values = {'n_estimators':[100,200,300],'eta':[0.07, 0.1, 0.13], 'max_depth':[3,4,5]}\n\nxgb = GridSearchCV(xgbcl, param_grid = grid_values, cv=2)\nxgb.fit(X_train, y_train)\n\nprint('XGBoost', xgb.best_score_, xgb.best_params_, time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T20:54:13.407149Z","iopub.execute_input":"2022-04-14T20:54:13.407727Z","iopub.status.idle":"2022-04-14T20:55:15.226501Z","shell.execute_reply.started":"2022-04-14T20:54:13.407691Z","shell.execute_reply":"2022-04-14T20:55:15.225667Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[20:54:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:54:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:55:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:55:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:55:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:55:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:55:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:55:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:55:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:55:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:55:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[20:55:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nXGBoost 0.7210222222222222 {'eta': 0.13, 'max_depth': 5, 'n_estimators': 300} 61.81111741065979\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}