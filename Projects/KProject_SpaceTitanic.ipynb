{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"##################################################################################################\n### This script is ML Classification template, which should be applicable to most MLC projects ###\n##################################################################################################\n\n\"\"\"Structure of the script:\n1. Load all needed libraries and functions.\n2. Load data, do preliminary data exploration.\n3. [Optional] Transform skewed variables.\n4. Trnasform features depending on their type. OHC.\n5. Create subsamples.\n6. Do scaling.\n7. Fit models, selecting hyperparameters via CV grid search.\n8. Evaluate performance of the selected models on test sample.\n\"\"\"\n# it is not the latest ML template. See HackerRank_ML_availability_final for later version.","metadata":{"execution":{"iopub.status.busy":"2022-03-30T18:43:42.424178Z","iopub.execute_input":"2022-03-30T18:43:42.425072Z","iopub.status.idle":"2022-03-30T18:43:42.456183Z","shell.execute_reply.started":"2022-03-30T18:43:42.424973Z","shell.execute_reply":"2022-03-30T18:43:42.455208Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'Structure of the script:\\n1. Load all needed libraries and functions.\\n2. Load data, do preliminary data exploration.\\n3. [Optional] Transform skewed variables.\\n4. Trnasform features depending on their type. OHC.\\n5. Create subsamples.\\n6. Do scaling.\\n7. Fit models, selecting hyperparameters via CV grid search.\\n8. Evaluate performance of the selected models on test sample.\\n'"},"metadata":{}}]},{"cell_type":"code","source":"### 1.Load main libraries ###\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\n\nfrom sklearn import svm\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom xgboost import XGBRegressor, XGBClassifier\n\npd.set_option('display.max_columns', 20)\npd.set_option('mode.chained_assignment', None)\npd.set_option('display.expand_frame_repr', False)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T18:43:44.687052Z","iopub.execute_input":"2022-03-30T18:43:44.688104Z","iopub.status.idle":"2022-03-30T18:43:46.044636Z","shell.execute_reply.started":"2022-03-30T18:43:44.688051Z","shell.execute_reply":"2022-03-30T18:43:46.043765Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def draw_histograms(df, variables, n_rows, n_cols):\n    # stolen from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n    fig=plt.figure()\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=10,ax=ax)\n        ax.set_title(var_name+\" Distribution\")\n    fig.tight_layout()  \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T18:43:49.252218Z","iopub.execute_input":"2022-03-30T18:43:49.252524Z","iopub.status.idle":"2022-03-30T18:43:49.258686Z","shell.execute_reply.started":"2022-03-30T18:43:49.252493Z","shell.execute_reply":"2022-03-30T18:43:49.257805Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"### 2.Load data ###\n\npath = '../input/titanic/train.csv'\ndf = pd.read_csv(path) # titanic_fullsample\nprint(df.shape)\ndf.drop(columns=['Name', 'Ticket', 'Cabin'],inplace=True)\ndf.loc[df.Age.isnull(),'Age'] = df.Age.median()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T18:49:17.557957Z","iopub.execute_input":"2022-03-30T18:49:17.558438Z","iopub.status.idle":"2022-03-30T18:49:17.621372Z","shell.execute_reply.started":"2022-03-30T18:49:17.558403Z","shell.execute_reply":"2022-03-30T18:49:17.620538Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(891, 12)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n0            1         0       3    male  22.0      1      0   7.2500        S\n1            2         1       1  female  38.0      1      0  71.2833        C\n2            3         1       3  female  26.0      0      0   7.9250        S\n3            4         1       1  female  35.0      1      0  53.1000        S\n4            5         0       3    male  35.0      0      0   8.0500        S","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_data=pd.read_csv('../input/hackerrank-availability/test (2).csv')\ntest_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data=pd.read_csv('../input/hackerrank-availability/test (2).csv')\nprint(test_data.head())\n\nprint(data.shape, test_data.shape)\ntrain = data.copy()\ntest = test_data.copy()\ntrain['sample']='train'\ntest['yearly_availability'] = np.nan\ntest['sample']='test'\nprint(train.columns==test.columns)\n# good, so we can concatenate train and test.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.describe())\n# sns.pairplot(df[['Survived', 'Pclass', 'Age', 'Fare']])\ndraw_histograms(df, df.columns, 4, 3)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T21:56:58.022901Z","iopub.execute_input":"2022-03-29T21:56:58.023453Z","iopub.status.idle":"2022-03-29T21:56:59.704116Z","shell.execute_reply.started":"2022-03-29T21:56:58.023382Z","shell.execute_reply":"2022-03-29T21:56:59.703142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%% 3.Transform some skewed variables ###\n\ndf['Fare'] = np.log1p(df.Fare)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T21:57:01.723803Z","iopub.execute_input":"2022-03-29T21:57:01.724449Z","iopub.status.idle":"2022-03-29T21:57:01.733444Z","shell.execute_reply.started":"2022-03-29T21:57:01.724380Z","shell.execute_reply":"2022-03-29T21:57:01.732033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%% 4.Transform features depending on their type ###\n\n# this is very important for ML application, where there are hundreds of features.\n# If there are less than 20 features, can use standard approach.\n# my approach of tackling one feature a time is not scalable. \n\n# use intuition to trim range or ordinary variables \n# can skip this step in general, since it is not scalable when number of features grows.\ndf.loc[df.SibSp>2,'SibSp']=3\ndf.loc[df.Parch>2,'Parch']=3\n\n# identify binary and categorical variables\ndf_uniques = pd.DataFrame([[i, len(df[i].unique())] for i in df.columns], columns=['Variable', 'Unique Values']).set_index('Variable')\nprint(df_uniques)\n\nbinary_variables = list(df_uniques[df_uniques['Unique Values'] == 2].index)\ncategorical_variables = list(df_uniques[(6 >= df_uniques['Unique Values']) & (df_uniques['Unique Values'] > 2)].index)\nnumeric_variables = list(set(df.columns) - set(categorical_variables) - set(binary_variables))\nprint('Binary variables are ', binary_variables)\nprint('Categorical variables are ', categorical_variables)\nprint('Numeric variables are ', numeric_variables)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T21:57:04.134387Z","iopub.execute_input":"2022-03-29T21:57:04.134923Z","iopub.status.idle":"2022-03-29T21:57:04.159781Z","shell.execute_reply.started":"2022-03-29T21:57:04.134884Z","shell.execute_reply":"2022-03-29T21:57:04.158619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ohc for binary variables #\nlb = LabelBinarizer()\nbinary_variables.remove('Survived')\nfor column in binary_variables:\n    df[column] = lb.fit_transform(df[column])\n\n# ohc for categorical variables #\ndf = pd.get_dummies(df, columns = categorical_variables, drop_first=True)\n\nprint(df.shape)\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2022-03-29T21:57:06.814313Z","iopub.execute_input":"2022-03-29T21:57:06.814701Z","iopub.status.idle":"2022-03-29T21:57:06.841241Z","shell.execute_reply.started":"2022-03-29T21:57:06.814669Z","shell.execute_reply":"2022-03-29T21:57:06.840222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %% 5.Creating subsamples ###\n\ny = df['Survived']\nX = df.drop(columns=['Survived'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T23:32:05.391992Z","iopub.execute_input":"2022-03-29T23:32:05.392355Z","iopub.status.idle":"2022-03-29T23:32:05.404867Z","shell.execute_reply.started":"2022-03-29T23:32:05.392312Z","shell.execute_reply":"2022-03-29T23:32:05.403497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %% 6.scaling numeric variables ###\n\ndraw_histograms(X_train, numeric_variables, 2, 3)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T23:32:14.496907Z","iopub.execute_input":"2022-03-29T23:32:14.497201Z","iopub.status.idle":"2022-03-29T23:32:15.035779Z","shell.execute_reply.started":"2022-03-29T23:32:14.497169Z","shell.execute_reply":"2022-03-29T23:32:15.034713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss = StandardScaler()\n\nfor column in [numeric_variables]:\n    X_train[column] = ss.fit_transform(X_train[column])\n    X_test[column] = ss.transform(X_test[column])","metadata":{"execution":{"iopub.status.busy":"2022-03-29T23:32:17.455783Z","iopub.execute_input":"2022-03-29T23:32:17.456526Z","iopub.status.idle":"2022-03-29T23:32:17.474878Z","shell.execute_reply.started":"2022-03-29T23:32:17.456491Z","shell.execute_reply":"2022-03-29T23:32:17.473734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################\n### 7.Fit models ###\n####################\n\n#%% Logistic regression ###\n\ngrid_values = {'penalty': ['l2'], 'C': list(np.arange(1,10.5,0.5))}\nlr = LogisticRegression()\nmodel_lr = GridSearchCV(lr, param_grid=grid_values, cv = 8)\nmodel_lr.fit(X_train, y_train)\nprint(model_lr.best_score_, model_lr.best_params_)\n\n# model_lr.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T23:32:20.074442Z","iopub.execute_input":"2022-03-29T23:32:20.074753Z","iopub.status.idle":"2022-03-29T23:32:23.410722Z","shell.execute_reply.started":"2022-03-29T23:32:20.074719Z","shell.execute_reply":"2022-03-29T23:32:23.407378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%% KNN ###\n\ngrid_values = dict(n_neighbors=np.arange(1,40))\nknnm = KNeighborsClassifier()   \nmodel_knn = GridSearchCV(knnm, param_grid=grid_values, cv = 8)\nmodel_knn.fit(X_train, y_train)\nprint(model_knn.best_score_, model_knn.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T23:32:28.344192Z","iopub.execute_input":"2022-03-29T23:32:28.344531Z","iopub.status.idle":"2022-03-29T23:32:32.468128Z","shell.execute_reply.started":"2022-03-29T23:32:28.344495Z","shell.execute_reply":"2022-03-29T23:32:32.467043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%% SVM ###\n\ngrid_values = {'C': np.arange(0.1, 3, 0.1)} \nsvmm = svm.SVC(kernel='rbf')\nmodel_svm = GridSearchCV(svmm, param_grid=grid_values, cv = 8)\nmodel_svm.fit(X_train, y_train)\nprint(model_svm.best_score_, model_svm.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T23:32:38.469905Z","iopub.execute_input":"2022-03-29T23:32:38.470539Z","iopub.status.idle":"2022-03-29T23:32:46.251993Z","shell.execute_reply.started":"2022-03-29T23:32:38.470503Z","shell.execute_reply":"2022-03-29T23:32:46.250968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%% RF ###\n\n# may look here: https://www.geeksforgeeks.org/hyperparameter-tuning/\n\ngrid_values = [{'max_depth': list(range(2, 11, 1)), 'max_features': list(np.arange(0.3,0.71,0.1))}]\nrfc = RandomForestClassifier(random_state=42)\nmodel_rf = GridSearchCV(rfc, grid_values, cv = 8, scoring='accuracy')\nmodel_rf.fit(X_train, y_train)\nprint(model_rf.best_score_, model_rf.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T23:33:37.248203Z","iopub.execute_input":"2022-03-29T23:33:37.248544Z","iopub.status.idle":"2022-03-29T23:34:45.167285Z","shell.execute_reply.started":"2022-03-29T23:33:37.248511Z","shell.execute_reply":"2022-03-29T23:34:45.166123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%% XGBoost ###\n# run this code only on Kaggle with GPU\n\nestimator = XGBClassifier(\n    nthread=4,\n    seed=42,\n    use_label_encoder=False\n)\n\nparameters = {\n    'max_depth': range (2, 4, 1),\n    'n_estimators': range(100, 301, 50),\n    'learning_rate': [0.001, 0.025, 0.05, 0.075]\n}\nt1 = time.time()\ngrid_search = GridSearchCV(\n    estimator=estimator,\n    param_grid=parameters,\n    scoring = 'roc_auc',\n    n_jobs = -1,\n    cv = 8,\n    verbose=True\n)\n\ngrid_search.fit(X_train, y_train, eval_metric='rmse')\nprint(grid_search.best_score_, grid_search.best_params_)\nprint(time.time()-t1)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T23:35:52.679667Z","iopub.execute_input":"2022-03-29T23:35:52.680001Z","iopub.status.idle":"2022-03-29T23:37:03.376767Z","shell.execute_reply.started":"2022-03-29T23:35:52.679968Z","shell.execute_reply":"2022-03-29T23:37:03.375733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param['gpu_id'] = 0\nparam['tree_method'] = 'gpu_hist'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbc = XGBClassifier(nthread=4, seed=42, use_label_encoder=False,\n                     max_depth=2, n_estimators=100, learning_rate=0.05)\nxgbc.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T23:37:45.620597Z","iopub.execute_input":"2022-03-29T23:37:45.620939Z","iopub.status.idle":"2022-03-29T23:37:45.737494Z","shell.execute_reply.started":"2022-03-29T23:37:45.620904Z","shell.execute_reply":"2022-03-29T23:37:45.736448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here I will add ANN\n\nfrom keras.models  import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n","metadata":{"execution":{"iopub.status.busy":"2022-03-29T23:37:49.235054Z","iopub.execute_input":"2022-03-29T23:37:49.235341Z","iopub.status.idle":"2022-03-29T23:37:49.248620Z","shell.execute_reply.started":"2022-03-29T23:37:49.235309Z","shell.execute_reply":"2022-03-29T23:37:49.246587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-29T23:37:51.232321Z","iopub.execute_input":"2022-03-29T23:37:51.232684Z","iopub.status.idle":"2022-03-29T23:37:51.239869Z","shell.execute_reply.started":"2022-03-29T23:37:51.232651Z","shell.execute_reply":"2022-03-29T23:37:51.238768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2 = Sequential()\nmodel_2.add(Dense(50, input_shape=(X_train.shape[1],), activation=\"relu\"))\nmodel_2.add(Dropout(0.4))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Dense(10, activation=\"relu\"))\nmodel_2.add(Dropout(0.4))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Dense(1, activation=\"sigmoid\"))\n\nes = EarlyStopping(monitor='val_loss', patience=20)\n\nmodel_2.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nrun_hist_2 = model_2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=300, callbacks=[es])","metadata":{"execution":{"iopub.status.busy":"2022-03-29T23:38:40.299005Z","iopub.execute_input":"2022-03-29T23:38:40.299351Z","iopub.status.idle":"2022-03-29T23:38:59.176828Z","shell.execute_reply.started":"2022-03-29T23:38:40.299303Z","shell.execute_reply":"2022-03-29T23:38:59.175794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# played a lot with ann. the best acc i can reliable achieve is 78.5%.\n# but the second time (test set) I get 83%...\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%% 8.Evaluate performance oos ###\n\nyhat_lm = model_lr.predict(X_test)\nyhat_knn = model_knn.predict(X_test)\nyhat_svm = model_svm.predict(X_test)\nyhat_rf = model_rf.predict(X_test)\nyhat_bt = xgbc.predict(X_test)\nprint('Accuracy of logistic regression is ', 1-(np.abs(yhat_lm-y_test)).mean())\nprint('Accuracy of KNN is ', 1-(np.abs(yhat_knn-y_test)).mean())\nprint('Accuracy of SVM is ', 1-(np.abs(yhat_svm-y_test)).mean())\nprint('Accuracy of RF is ', 1-(np.abs(yhat_rf-y_test)).mean())\nprint('Accuracy of Boosted Tree is ', 1-(np.abs(yhat_bt-y_test)).mean())","metadata":{"execution":{"iopub.status.busy":"2022-03-29T23:39:35.862892Z","iopub.execute_input":"2022-03-29T23:39:35.863211Z","iopub.status.idle":"2022-03-29T23:39:35.929163Z","shell.execute_reply.started":"2022-03-29T23:39:35.863177Z","shell.execute_reply":"2022-03-29T23:39:35.928084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Export results ###\nyhat_bt = boosted_tree.predict(X_test)\nyhat_bt[0:20]\n\nsubmissions_df = pd.DataFrame({'id': test.id, 'yearly_availability': yhat_bt}, columns=['id', 'yearly_availability'])\nsubmissions_df\n\nsubmissions_df.to_csv('submissions.csv',index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}