{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nCreated on Mon Feb  7 19:08:38 2022\nUpdated script for titanic, more pythonic version\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBRegressor\n\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, MinMaxScaler\n\n\n\npd.set_option('display.max_columns', 20)\npd.set_option('mode.chained_assignment', None)\nos.chdir(\"H:/Dropbox/Kaggle/titanic\")\n\n\n#%% Import data ###\n\ndf = pd.read_csv(\"train.csv\") # titanic_fullsample\nprint(df.head())\nprint(df.shape)\n\n#%% Encode categorical variables ###\n\ndf.drop(columns=['Name', 'Ticket', 'Cabin'],inplace=True)\ndf.loc[df.Age.isnull(),'Age'] = df.Age.median()\n\ndf.loc[df.SibSp>2,'SibSp']=3\ndf.loc[df.Parch>2,'Parch']=3\n\ndf_uniques = pd.DataFrame([[i, len(df[i].unique())] for i in df.columns], columns=['Variable', 'Unique Values']).set_index('Variable')\ndf_uniques\n\nbinary_variables = list(df_uniques[df_uniques['Unique Values'] == 2].index)\ncategorical_variables = list(df_uniques[(6 >= df_uniques['Unique Values']) & (df_uniques['Unique Values'] > 2)].index)\nnumeric_variables = list(set(df.columns) - set(categorical_variables) - set(binary_variables))\n\nlb = LabelBinarizer()\nbinary_variables.remove('Survived')\n\nfor column in binary_variables:\n    df[column] = lb.fit_transform(df[column])\n\ndf = pd.get_dummies(df, columns = categorical_variables, drop_first=True)\n\nprint(df.head(10))\n\n# %% scaling ###\n\ny = df['Survived']\nX = df.drop(columns=['Survived'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmm = MinMaxScaler()\n\nfor column in [numeric_variables]:\n    X_train[column] = mm.fit_transform(X_train[column])\n    X_test[column] = mm.fit_transform(X_test[column])\n\n#%% Logistic regression ###\n\ngrid_values = {'penalty': ['l2'], 'C': [1,2,4,10,15,20,25,30,40,100]}\nlr = LogisticRegression()\nmodel_lr = GridSearchCV(lr, param_grid=grid_values, cv = 20)\nmodel_lr.fit(X_train, y_train)\nprint(model_lr.best_score_, model_lr.best_params_)\n\n# model_lr.predict(X_test)\n\n#%% KNN ###\n\ngrid_values = dict(n_neighbors=np.arange(1,40))\nknnm = KNeighborsClassifier()   \nmodel_knn = GridSearchCV(knnm, param_grid=grid_values, cv = 20)\nmodel_knn.fit(X_train, y_train)\nprint(model_knn.best_score_, model_knn.best_params_)\n\n\n#%% SVM ###\n\ngrid_values = {'C': np.arange(0.05, 1, 0.05)} \nsvmm = svm.SVC(kernel='rbf')\nmodel_svm = GridSearchCV(svmm, param_grid=grid_values, cv = 20)\nmodel_svm.fit(X_train, y_train)\nprint(model_svm.best_score_, model_svm.best_params_)\n\n#%% RF ###\n\n# may look here: https://www.geeksforgeeks.org/hyperparameter-tuning/\n\n\ngrid_values = [{'max_depth': list(range(2, 9)), 'max_features': list(np.arange(0.2,0.71,0.05))}]\nrfc = RandomForestClassifier(random_state=42)\nmodel_rf = GridSearchCV(rfc, grid_values, cv = 20, scoring='accuracy')\nmodel_rf.fit(X_train, y_train)\nprint(model_rf.best_score_, model_rf.best_params_)\n\n\n#%% XGBoost? ###\n# run this code only on Kaggle with GPU\n# see kaggle for updated code\n\nestimator = XGBClassifier(\n    objective= 'binary:logistic',\n    nthread=4,\n    seed=42\n)\n\nparameters = {\n    'max_depth': range (2, 4, 1),\n    'n_estimators': range(20, 50),\n    'learning_rate': [0.05, 0.1]\n}\n\ngrid_search = GridSearchCV(\n    estimator=estimator,\n    param_grid=parameters,\n    scoring = 'roc_auc',\n    n_jobs = 10,\n    cv = 10,\n    verbose=True\n)\n\ngrid_search.fit(X_train, y_train)\nprint(grid_search.best_score_, grid_search.best_params_)\n\n\n\n\n\n#%%  ###\n\n\nA = np.arange(0.6, 1.01, 0.01)\nmae_logm_ar = np.zeros(len(A))\n\nfor i in np.arange(len(A)):\n    a = A[i]\n    logm = LogisticRegression(C=a, solver='liblinear')\n    yhat_logm = cross_val_predict(logm, X, y, cv=10)\n    mae_logm_ar[i] = np.mean(np.abs(np.array(y)-yhat_logm))\n\nmae_a = pd.DataFrame({'a': A, 'mae': mae_logm_ar}, columns=['a', 'mae'])\nprint(mae_a)\n\n# C=1 seems to work best, 18.5% mae.\n\nfullmodel = LogisticRegression(C=1, solver='liblinear')\nfullmodel.fit(X,y)\n\n#%%  ###\n\n\nX = preprocessing.StandardScaler().fit(X).transform(X.astype(float))\n\nA = np.arange(10,40)\nmae_knnm_ar = np.zeros(len(A))\n\nfor i in np.arange(len(A)):\n    a = A[i]\n    knnm = KNeighborsClassifier(n_neighbors = a)    \n    yhat_knnm = cross_val_predict(knnm, X, y, cv=45)\n    mae_knnm_ar[i] = np.mean(np.abs(np.array(y)-yhat_knnm))\n\nmae_a = pd.DataFrame({'a': A, 'mae': mae_knnm_ar}, columns=['a', 'mae'])\nprint(mae_a)\n\n# k=30 seems the best, mae aroun 20.2%.\n\n#%%  ###\n\n\nrfm = RandomForestRegressor(random_state=1, max_depth=12)\n# rfm = RandomForestRegressor(random_state=1, max_depth=12, max_features='sqrt')\n\n\nyhat_rfm = cross_val_predict(rfm, X, y, cv=25)\nmae_rfm = np.mean(np.abs(np.array(y)-yhat_rfm))\nprint(mae_rfm)\n\n# max_depth=12 seems optimal, mae=25.8%.\n\n#%%  ###\n\n\nA = np.arange(0.3, 1.1, 0.1)\nmae_svmm_ar = np.zeros(len(A))\n\nfor i in np.arange(len(A)):\n    a = A[i]\n    svmm = svm.SVC(C=a, kernel='rbf')\n    yhat_svmm = cross_val_predict(svmm, X, y, cv=21)\n    mae_svmm_ar[i] = np.mean(np.abs(np.array(y)-yhat_svmm))\n\nmae_a = pd.DataFrame({'a': A, 'mae': mae_svmm_ar}, columns=['a', 'mae'])\nprint(mae_a)\n\n# at C=0.4 mae is 18.7%\n\nfullmodel = svm.SVC(C=0.4, kernel='rbf')\nfullmodel.fit(X,y)\n\n\n\n\n\n\n\n\n\n\n\n\n#%%  ###\n\n\ntests = pd.read_csv(\"test.csv\") # titanic_fullsample\nprint(tests.head())\nprint(tests.shape)\n\n\ntests.drop(columns=['Name', 'Ticket'],inplace=True)\ntests['Sex']=(tests['Sex']=='male')*1\ntests['Cabin']= ~ (tests.Cabin.isna())*1\nprint(tests.head(10))\nprint(tests.describe(include='all'))\n\n\ntests['Parch0']=(tests['Parch']==0)*1\ntests['Parch1']=(tests['Parch']==1)*1\ntests['Parch2']=(tests['Parch']==2)*1\ntests['Parch3']=(tests['Parch']>2)*1\n\ntests['SibSp0']=(tests['SibSp']==0)*1\ntests['SibSp1']=(tests['SibSp']==1)*1\ntests['SibSp2']=(tests['SibSp']==2)*1\ntests['SibSp3']=(tests['SibSp']>2)*1\n\ntests['EmbarkedS']=(tests['Embarked']=='S')*1\ntests['EmbarkedC']=(tests['Embarked']=='C')*1\ntests['EmbarkedQ']=(tests['Embarked']=='Q')*1\n\ntests['Pclass1']=(tests['Pclass']==1)*1\ntests['Pclass2']=(tests['Pclass']==2)*1\ntests['Pclass3']=(tests['Pclass']==3)*1\n\ntests.drop(columns=['Parch', 'SibSp', 'Embarked', 'Pclass'],inplace=True)\n\ntests['Age'][tests.Age.isna()]=tests.Age.mean()\n\nprint(tests.head(10))\n\n#%%  ###\n\n\nX = tests.copy()\nX.loc[X.Fare.isna(),'Fare'] = X.Fare.mean()\n\nyhat = fullmodel.predict(X)\nthreshold = np.quantile(yhat, 1-y.mean()-a)\nyhat[yhat>threshold]=1\nyhat[yhat<=threshold]=0\nyhat = yhat.astype(int)\n\nresults = pd.DataFrame({'PassengerId': tests.PassengerId, 'Survived': yhat}, columns=['PassengerId', 'Survived'])\n\nresults.to_csv('Titanic_subm6.csv', index=False)  \n\n\n\n\n\n\n\n\n\n\n\n\n## aside:creating binary variables.     df['weekend'] = df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)\n##                                      df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)\n## creating dummies                     Feature = pd.concat([Feature,pd.get_dummies(df['education'])], axis=1)\n\n#%%  ###\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}
