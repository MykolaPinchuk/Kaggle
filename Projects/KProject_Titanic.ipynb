{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"##################################################################################################\n### This script is ML Classification template, which should be applicable to most MLC projects ###\n##################################################################################################\n\n# In v_14 I merge all cells into a single one.\n\n\"\"\"Structure of the script:\n1. Load all needed libraries and functions.\n2. Load data, do preliminary data exploration, deal with missing values.\n3. [Optional] Transform skewed variables.\n4. Transform features depending on their type. OHC.\n5. Create subsamples.\n6. Do scaling.\n7. Fit models, selecting hyperparameters via CV grid search.\n8. Evaluate performance of the selected models on test sample.\n\"\"\"\n# it is not the latest ML template. See HackerRank_ML_availability_final for later version.\n\n### 1.Load main libraries ###\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\n\nfrom sklearn import svm\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom xgboost import XGBRegressor, XGBClassifier\n\npd.set_option('display.max_columns', 20)\npd.set_option('mode.chained_assignment', None)\npd.set_option('display.expand_frame_repr', False)\n\ndef draw_histograms(df, variables, n_rows, n_cols):\n    # stolen from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n    fig=plt.figure()\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=10,ax=ax)\n        ax.set_title(var_name+\" Distribution\")\n    fig.tight_layout()  \n    plt.show()\n\n### 2.Load data ###\n\ntime1 = time.time()\n\npath = '../input/titanic/train.csv'\ndf = pd.read_csv(path) # titanic_fullsample\nprint(df.shape)\ndf.drop(columns=['Name', 'Ticket', 'Cabin'],inplace=True)\ndf.head()\n\ntest_data=pd.read_csv('../input/titanic/test.csv')\ntest_data.drop(columns=['Name', 'Ticket', 'Cabin'],inplace=True)\ntest_data.head()\n\nprint(df.shape, test_data.shape)\ntrain = df.copy()\ntest = test_data.copy()\ntrain['sample']='train'\ntest['Survived'] = np.nan\ntest['sample']='test'\n\ndf=pd.concat([train, test])\ndf.reset_index(inplace=True, drop=True)\nprint(df.shape)\ndf.tail(3)\n\n# deal with mising values\n\ndf.loc[df.Age.isnull(),'Age'] = df.Age.median()\ndf.loc[df.Fare.isnull(),'Fare'] = df.Fare.median()\nprint(df.describe())\n# sns.pairplot(df[['Survived', 'Pclass', 'Age', 'Fare']])\ndraw_histograms(df, df.columns, 4, 3)\n\n#%% 3.Transform some skewed variables ###\n\ndf['Fare'] = np.log1p(df.Fare)\n\n#%% 4.Transform features depending on their type ###\n\n# this is very important for ML application, where there are hundreds of features.\n# If there are less than 20 features, can use standard approach.\n# my approach of tackling one feature a time is not scalable. \n\n# use intuition to trim range or ordinary variables \n# can skip this step in general, since it is not scalable when number of features grows.\ndf.loc[df.SibSp>2,'SibSp']=3\ndf.loc[df.Parch>2,'Parch']=3\n\n# identify binary and categorical variables\ndf_uniques = pd.DataFrame([[i, len(df[i].unique())] for i in df.columns], columns=['Variable', 'Unique Values']).set_index('Variable')\ndf_uniques.drop('Survived', inplace = True)\nprint(df_uniques)\n\nbinary_variables = list(df_uniques[df_uniques['Unique Values'] == 2].index)\ncategorical_variables = list(df_uniques[(6 >= df_uniques['Unique Values']) & (df_uniques['Unique Values'] > 2)].index)\nnumeric_variables = list(set(df.columns) - set(categorical_variables) - set(binary_variables) - set(['Survived']))\nprint('Binary variables are ', binary_variables)\nprint('Categorical variables are ', categorical_variables)\nprint('Numeric variables are ', numeric_variables)\n\n# ohc for binary variables #\nlb = LabelBinarizer()\nbinary_variables.remove('sample')\nfor column in binary_variables:\n    df[column] = lb.fit_transform(df[column])\n\n# ohc for categorical variables #\ndf = pd.get_dummies(df, columns = categorical_variables, drop_first=True)\n\nprint(df.shape)\nprint(df.head())\n\n# %% 5.Creating subsamples ###\n\ntrain = df[df['sample']=='train'].copy()\ntrain.drop(columns=['sample'], inplace=True)\ntest = df[df['sample']=='test'].copy()\ntest.drop(columns=['sample'], inplace=True)\n\nprint(train.shape)\nprint(test.shape)\ntrain.head(3)\n\ny_train = train['Survived']\nX_train = train.drop(columns=['Survived'])\nX_test = test.drop(columns=['Survived'])\n\nprint(X_train.shape)\nX_train.head(3)\n\nX_train, X_traintest, y_train, y_traintest = train_test_split(X_train,y_train,test_size=0.2, random_state=8)\n\nprint(X_train.shape)\nprint(X_traintest.shape)\nprint(X_test.shape)\nX_traintest.head(3)\n\n# 'traintest' is hold-out sample to veify that chosen model indeed works.\n# it is different from 'test', which is truly out of sample.","metadata":{"execution":{"iopub.status.busy":"2022-04-01T16:26:50.892194Z","iopub.execute_input":"2022-04-01T16:26:50.892538Z","iopub.status.idle":"2022-04-01T16:26:53.944561Z","shell.execute_reply.started":"2022-04-01T16:26:50.892455Z","shell.execute_reply":"2022-04-01T16:26:53.943614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %% 6.scaling numeric variables ###\n\ndraw_histograms(X_train, numeric_variables, 2, 3)\n\nss = StandardScaler()\n\nfor column in [numeric_variables]:\n    X_train[column] = ss.fit_transform(X_train[column])\n    X_traintest[column] = ss.transform(X_traintest[column])\n    X_test[column] = ss.transform(X_test[column])","metadata":{"execution":{"iopub.status.busy":"2022-04-01T16:28:22.99024Z","iopub.execute_input":"2022-04-01T16:28:22.990521Z","iopub.status.idle":"2022-04-01T16:28:23.436368Z","shell.execute_reply.started":"2022-04-01T16:28:22.990493Z","shell.execute_reply":"2022-04-01T16:28:23.435566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################\n### 7.Fit models ###\n####################\n\ntime1_1 = time.time()\n\n#%% Logistic regression ###\n\ngrid_values = {'penalty': ['l2'], 'C': list(np.arange(1,10.5,0.5))}\nlr = LogisticRegression()\nmodel_lr = GridSearchCV(lr, param_grid=grid_values, cv = 8)\nmodel_lr.fit(X_train, y_train)\nprint('logistic ', model_lr.best_score_, model_lr.best_params_)\n\n# model_lr.predict(X_test)\n\n#%% KNN ###\n\ngrid_values = dict(n_neighbors=np.arange(5,25))\nknnm = KNeighborsClassifier()   \nmodel_knn = GridSearchCV(knnm, param_grid=grid_values, cv = 8)\nmodel_knn.fit(X_train, y_train)\nprint('knn ', model_knn.best_score_, model_knn.best_params_)\n\n#%% SVM ###\n\ngrid_values = {'C': np.arange(0.1, 4, 0.2)} \nsvmm = svm.SVC(kernel='rbf')\nmodel_svm = GridSearchCV(svmm, param_grid=grid_values, cv = 8)\nmodel_svm.fit(X_train, y_train)\nprint('svm ', model_svm.best_score_, model_svm.best_params_)\n\n#%% RF ###\n\n# may look here: https://www.geeksforgeeks.org/hyperparameter-tuning/\n\ngrid_values = [{'max_depth': list(range(3, 7, 1)), 'max_features': list(np.arange(0.2,0.51,0.1)),\n               'n_estimators': [100, 200]}]\nrfc = RandomForestClassifier(random_state=42)\nmodel_rf = GridSearchCV(rfc, grid_values, cv = 8, scoring='accuracy')\nmodel_rf.fit(X_train, y_train)\nprint('rf ', model_rf.best_score_, model_rf.best_params_)\nprint('4 models time ', time.time()-time1_1)\n\n#%% XGBoost ###\n# run this code only on Kaggle with GPU\n\ntime2 = time.time()\n\nestimator = XGBClassifier(\n    nthread=4,\n    seed=42,\n    use_label_encoder=False\n)\n\nparameters = {\n    'max_depth': range (2, 4, 1),\n    'n_estimators': range(100, 301, 50),\n    'learning_rate': [0.01, 0.03, 0.05]\n}\nt2 = time.time()\ngrid_search = GridSearchCV(\n    estimator=estimator,\n    param_grid=parameters,\n    scoring = 'roc_auc',\n    n_jobs = -1,\n    cv = 8,\n    verbose=True\n)\n\ngrid_search.fit(X_train, y_train, eval_metric='rmse')\nprint('bt ', grid_search.best_score_, grid_search.best_params_)\nprint('bt time ', time.time()-t2)\n\nxgbc = XGBClassifier(nthread=4, seed=42, use_label_encoder=False,\n                     max_depth=2, n_estimators=300, learning_rate=0.01)\nxgbc.fit(X_train, y_train)\n\n# here I will add ANN\n\nfrom keras.models  import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel_2 = Sequential()\nmodel_2.add(Dense(50, input_shape=(X_train.shape[1],), activation=\"relu\"))\nmodel_2.add(Dropout(0.4))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Dense(10, activation=\"relu\"))\nmodel_2.add(Dropout(0.4))\nmodel_2.add(BatchNormalization())\nmodel_2.add(Dense(1, activation=\"sigmoid\"))\n\nes = EarlyStopping(monitor='val_loss', patience=10)\n\nmodel_2.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[\"accuracy\"])run_hist_2 = model_2.fit(X_train, y_train, validation_data=(X_traintest, y_traintest), epochs=300, callbacks=[es])\n\n# played a lot with ann. the best acc i can reliable achieve is 78.5%.\n# but the second time (test set) I get 83%...\n# usually its is competition btw rf and bt at the end. \n\n#%% 8.Evaluate performance oos ###\n\nyhat_lm = model_lr.predict(X_traintest)\nyhat_knn = model_knn.predict(X_traintest)\nyhat_svm = model_svm.predict(X_traintest)\nyhat_rf = model_rf.predict(X_traintest)\nyhat_btcv = grid_search.predict(X_traintest)\nyhat_btm = xgbc.predict(X_traintest)\nprint('Accuracy of logistic regression is ', 1-(np.abs(yhat_lm-y_traintest)).mean())\nprint('Accuracy of KNN is ', 1-(np.abs(yhat_knn-y_traintest)).mean())\nprint('Accuracy of SVM is ', 1-(np.abs(yhat_svm-y_traintest)).mean())\nprint('Accuracy of RF is ', 1-(np.abs(yhat_rf-y_traintest)).mean())\nprint('Accuracy of Boosted Tree cv is ', 1-(np.abs(yhat_btcv-y_traintest)).mean())\nprint('Accuracy of Boosted Tree manl is ', 1-(np.abs(yhat_btm-y_traintest)).mean())\nprint('total time is ', time.time() - time1)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T20:40:49.73852Z","iopub.execute_input":"2022-03-30T20:40:49.738779Z","iopub.status.idle":"2022-03-30T20:43:12.674653Z","shell.execute_reply.started":"2022-03-30T20:40:49.738751Z","shell.execute_reply":"2022-03-30T20:43:12.673729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Export results ###\nyhat_lr = model_lr.predict(X_test).astype(int)\nyhat_rf = model_rf.predict(X_test).astype(int)\nyhat_bt = grid_search.predict(X_test).astype(int)\n\nsubmission_df_lr = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': yhat_lr}, columns=['PassengerId', 'Survived'])\nsubmission_df_rf = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': yhat_rf}, columns=['PassengerId', 'Survived'])\nsubmission_df_bt = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': yhat_bt}, columns=['PassengerId', 'Survived'])\n\nsubmission_df_lr.to_csv('submissions_Titanic_i10_lr1.csv',index=False)\nsubmission_df_rf.to_csv('submissions_Titanic_i10_rf1.csv',index=False)\nsubmission_df_bt.to_csv('submissions_Titanic_i10_bt1.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T20:45:25.531911Z","iopub.execute_input":"2022-03-30T20:45:25.532668Z","iopub.status.idle":"2022-03-30T20:45:25.580846Z","shell.execute_reply.started":"2022-03-30T20:45:25.532633Z","shell.execute_reply":"2022-03-30T20:45:25.58018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(r'/kaggle/working')\n\nfrom IPython.display import FileLink\nFileLink(r'submissions_Titanic_i10_lr1.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T20:45:31.936514Z","iopub.execute_input":"2022-03-30T20:45:31.937279Z","iopub.status.idle":"2022-03-30T20:45:31.943507Z","shell.execute_reply.started":"2022-03-30T20:45:31.937233Z","shell.execute_reply":"2022-03-30T20:45:31.942753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(r'submissions_Titanic_i10_rf1.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T20:45:33.457126Z","iopub.execute_input":"2022-03-30T20:45:33.457573Z","iopub.status.idle":"2022-03-30T20:45:33.462957Z","shell.execute_reply.started":"2022-03-30T20:45:33.457538Z","shell.execute_reply":"2022-03-30T20:45:33.462003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(r'submissions_Titanic_i10_bt1.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T20:45:34.82614Z","iopub.execute_input":"2022-03-30T20:45:34.826553Z","iopub.status.idle":"2022-03-30T20:45:34.832563Z","shell.execute_reply.started":"2022-03-30T20:45:34.826517Z","shell.execute_reply":"2022-03-30T20:45:34.831857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat_lr.dtype","metadata":{"execution":{"iopub.status.busy":"2022-03-30T20:23:50.107735Z","iopub.execute_input":"2022-03-30T20:23:50.109757Z","iopub.status.idle":"2022-03-30T20:23:50.114755Z","shell.execute_reply.started":"2022-03-30T20:23:50.109721Z","shell.execute_reply":"2022-03-30T20:23:50.113987Z"},"trusted":true},"execution_count":null,"outputs":[]}]}